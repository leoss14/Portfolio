{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c89fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COLOR EXTRACTION  (adaptive domain handling)\n",
      "======================================================================\n",
      "Timeout: 15s | Retries: 1 (transient only)\n",
      "Adaptive thresholds: HEAD-check < 40% success, skip < 5% success (after 30 samples)\n",
      "\n",
      "Existing cache: 1,163 products — will resume.\n",
      "\n",
      "STEP 1: Loading data...\n",
      "  Loaded 21,436 products\n",
      "\n",
      "STEP 2: Filtering categories...\n",
      "  Remaining: 12,832 products\n",
      "\n",
      "STEP 3: Gender labels...\n",
      "  none: 10,913\n",
      "  female: 1,075\n",
      "  male: 844\n",
      "\n",
      "STEP 4: Selecting products...\n",
      "  Domain breakdown:\n",
      "    digitalcontent.api.tesco.com: 6,973\n",
      "    groceries.morrisons.com: 5,517\n",
      "    ui.assets-asda.com:443: 342\n",
      "  To process (after resume filter): 11,669\n",
      "\n",
      "STEP 5: Extracting colors (top 3 per image)...\n",
      "\n",
      "  1/11,669 | OK: 0 (0%) | 92.7 img/s | ETA: 2 min\n",
      "  [domain tracker] HEAD pre-checking digitalcontent.api.tesco.com (success rate 33% after 30 attempts)\n",
      "  100/11,669 | OK: 10 (10%) | 5.2 img/s | ETA: 37 min\n",
      "  200/11,669 | OK: 10 (5%) | 7.5 img/s | ETA: 25 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  [domain tracker] Skipping digitalcontent.api.tesco.com (success rate 5% after 201 attempts)\n",
      "  300/11,669 | OK: 10 (3%) | 11.2 img/s | ETA: 17 min\n",
      "  400/11,669 | OK: 10 (2%) | 15.0 img/s | ETA: 13 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  500/11,669 | OK: 10 (2%) | 18.7 img/s | ETA: 10 min\n",
      "  600/11,669 | OK: 10 (2%) | 22.4 img/s | ETA: 8 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  700/11,669 | OK: 10 (1%) | 26.2 img/s | ETA: 7 min\n",
      "  800/11,669 | OK: 10 (1%) | 29.9 img/s | ETA: 6 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  900/11,669 | OK: 10 (1%) | 33.6 img/s | ETA: 5 min\n",
      "  1,000/11,669 | OK: 10 (1%) | 37.4 img/s | ETA: 5 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  1,100/11,669 | OK: 10 (1%) | 41.1 img/s | ETA: 4 min\n",
      "  1,200/11,669 | OK: 10 (1%) | 44.8 img/s | ETA: 4 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  1,300/11,669 | OK: 10 (1%) | 48.5 img/s | ETA: 4 min\n",
      "  1,400/11,669 | OK: 10 (1%) | 52.2 img/s | ETA: 3 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  1,500/11,669 | OK: 10 (1%) | 55.9 img/s | ETA: 3 min\n",
      "  1,600/11,669 | OK: 10 (1%) | 59.7 img/s | ETA: 3 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  1,700/11,669 | OK: 10 (1%) | 63.4 img/s | ETA: 3 min\n",
      "  1,800/11,669 | OK: 10 (1%) | 67.1 img/s | ETA: 2 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  1,900/11,669 | OK: 10 (1%) | 70.8 img/s | ETA: 2 min\n",
      "  2,000/11,669 | OK: 10 (0%) | 74.5 img/s | ETA: 2 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  2,100/11,669 | OK: 10 (0%) | 78.2 img/s | ETA: 2 min\n",
      "  2,200/11,669 | OK: 10 (0%) | 81.9 img/s | ETA: 2 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  2,300/11,669 | OK: 10 (0%) | 85.6 img/s | ETA: 2 min\n",
      "  2,400/11,669 | OK: 10 (0%) | 89.3 img/s | ETA: 2 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  2,500/11,669 | OK: 10 (0%) | 93.0 img/s | ETA: 2 min\n",
      "  2,600/11,669 | OK: 10 (0%) | 96.7 img/s | ETA: 2 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  2,700/11,669 | OK: 10 (0%) | 100.3 img/s | ETA: 1 min\n",
      "  2,800/11,669 | OK: 10 (0%) | 104.0 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  2,900/11,669 | OK: 10 (0%) | 107.7 img/s | ETA: 1 min\n",
      "  3,000/11,669 | OK: 10 (0%) | 111.4 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  3,100/11,669 | OK: 10 (0%) | 115.1 img/s | ETA: 1 min\n",
      "  3,200/11,669 | OK: 10 (0%) | 118.8 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  3,300/11,669 | OK: 10 (0%) | 122.4 img/s | ETA: 1 min\n",
      "  3,400/11,669 | OK: 10 (0%) | 126.1 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  3,500/11,669 | OK: 10 (0%) | 129.8 img/s | ETA: 1 min\n",
      "  3,600/11,669 | OK: 10 (0%) | 133.5 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  3,700/11,669 | OK: 10 (0%) | 137.1 img/s | ETA: 1 min\n",
      "  3,800/11,669 | OK: 10 (0%) | 140.8 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  3,900/11,669 | OK: 10 (0%) | 144.5 img/s | ETA: 1 min\n",
      "  4,000/11,669 | OK: 10 (0%) | 148.1 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  4,100/11,669 | OK: 10 (0%) | 151.8 img/s | ETA: 1 min\n",
      "  4,200/11,669 | OK: 10 (0%) | 155.5 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  4,300/11,669 | OK: 10 (0%) | 159.1 img/s | ETA: 1 min\n",
      "  4,400/11,669 | OK: 10 (0%) | 162.8 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  4,500/11,669 | OK: 10 (0%) | 166.4 img/s | ETA: 1 min\n",
      "  4,600/11,669 | OK: 10 (0%) | 170.1 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  4,700/11,669 | OK: 10 (0%) | 173.7 img/s | ETA: 1 min\n",
      "  4,800/11,669 | OK: 10 (0%) | 177.4 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  4,900/11,669 | OK: 10 (0%) | 181.0 img/s | ETA: 1 min\n",
      "  5,000/11,669 | OK: 10 (0%) | 184.7 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  5,100/11,669 | OK: 10 (0%) | 188.3 img/s | ETA: 1 min\n",
      "  5,200/11,669 | OK: 10 (0%) | 192.0 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  5,300/11,669 | OK: 10 (0%) | 194.4 img/s | ETA: 1 min\n",
      "  5,400/11,669 | OK: 10 (0%) | 198.1 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  5,500/11,669 | OK: 10 (0%) | 201.6 img/s | ETA: 1 min\n",
      "  5,600/11,669 | OK: 10 (0%) | 205.3 img/s | ETA: 0 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  5,700/11,669 | OK: 10 (0%) | 208.9 img/s | ETA: 0 min\n",
      "  5,800/11,669 | OK: 10 (0%) | 212.5 img/s | ETA: 0 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  5,900/11,669 | OK: 10 (0%) | 216.1 img/s | ETA: 0 min\n",
      "  6,000/11,669 | OK: 10 (0%) | 219.7 img/s | ETA: 0 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  6,100/11,669 | OK: 10 (0%) | 223.3 img/s | ETA: 0 min\n",
      "  6,200/11,669 | OK: 10 (0%) | 226.9 img/s | ETA: 0 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  6,300/11,669 | OK: 10 (0%) | 230.5 img/s | ETA: 0 min\n",
      "  6,400/11,669 | OK: 10 (0%) | 234.1 img/s | ETA: 0 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  6,500/11,669 | OK: 10 (0%) | 237.7 img/s | ETA: 0 min\n",
      "  6,600/11,669 | OK: 10 (0%) | 241.3 img/s | ETA: 0 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  6,700/11,669 | OK: 10 (0%) | 244.8 img/s | ETA: 0 min\n",
      "  6,800/11,669 | OK: 10 (0%) | 248.5 img/s | ETA: 0 min\n",
      "  [checkpoint] 1,173 products in cache\n",
      "  6,900/11,669 | OK: 29 (0%) | 224.3 img/s | ETA: 0 min\n",
      "  7,000/11,669 | OK: 129 (2%) | 141.5 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,293 products in cache\n",
      "  7,100/11,669 | OK: 229 (3%) | 104.7 img/s | ETA: 1 min\n",
      "  7,200/11,669 | OK: 329 (5%) | 83.0 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,493 products in cache\n",
      "  7,300/11,669 | OK: 429 (6%) | 68.7 img/s | ETA: 1 min\n",
      "  7,400/11,669 | OK: 529 (7%) | 58.6 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,693 products in cache\n",
      "  7,500/11,669 | OK: 629 (8%) | 51.4 img/s | ETA: 1 min\n",
      "  7,600/11,669 | OK: 729 (10%) | 46.1 img/s | ETA: 1 min\n",
      "  [checkpoint] 1,893 products in cache\n",
      "  7,700/11,669 | OK: 829 (11%) | 41.8 img/s | ETA: 2 min\n",
      "  7,800/11,669 | OK: 929 (12%) | 38.4 img/s | ETA: 2 min\n",
      "  [checkpoint] 2,093 products in cache\n",
      "  7,900/11,669 | OK: 1029 (13%) | 35.6 img/s | ETA: 2 min\n",
      "  8,000/11,669 | OK: 1129 (14%) | 33.0 img/s | ETA: 2 min\n",
      "  [checkpoint] 2,293 products in cache\n",
      "  8,100/11,669 | OK: 1229 (15%) | 30.8 img/s | ETA: 2 min\n",
      "  8,200/11,669 | OK: 1329 (16%) | 29.0 img/s | ETA: 2 min\n",
      "  [checkpoint] 2,493 products in cache\n",
      "  8,300/11,669 | OK: 1429 (17%) | 27.5 img/s | ETA: 2 min\n",
      "  8,400/11,669 | OK: 1529 (18%) | 26.1 img/s | ETA: 2 min\n",
      "  [checkpoint] 2,693 products in cache\n",
      "  8,500/11,669 | OK: 1629 (19%) | 24.9 img/s | ETA: 2 min\n",
      "  8,600/11,669 | OK: 1729 (20%) | 23.7 img/s | ETA: 2 min\n",
      "  [checkpoint] 2,893 products in cache\n",
      "  8,700/11,669 | OK: 1829 (21%) | 22.8 img/s | ETA: 2 min\n",
      "  8,800/11,669 | OK: 1929 (22%) | 21.9 img/s | ETA: 2 min\n",
      "  [checkpoint] 3,093 products in cache\n",
      "  8,900/11,669 | OK: 2029 (23%) | 21.2 img/s | ETA: 2 min\n",
      "  9,000/11,669 | OK: 2129 (24%) | 20.6 img/s | ETA: 2 min\n",
      "  [checkpoint] 3,292 products in cache\n",
      "  9,100/11,669 | OK: 2229 (24%) | 20.0 img/s | ETA: 2 min\n",
      "  9,200/11,669 | OK: 2329 (25%) | 19.3 img/s | ETA: 2 min\n",
      "  [checkpoint] 3,492 products in cache\n",
      "  9,300/11,669 | OK: 2429 (26%) | 18.8 img/s | ETA: 2 min\n",
      "  9,400/11,669 | OK: 2529 (27%) | 18.3 img/s | ETA: 2 min\n",
      "  [checkpoint] 3,692 products in cache\n",
      "  9,500/11,669 | OK: 2629 (28%) | 17.9 img/s | ETA: 2 min\n",
      "  9,600/11,669 | OK: 2729 (28%) | 17.5 img/s | ETA: 2 min\n",
      "  [checkpoint] 3,892 products in cache\n",
      "  9,700/11,669 | OK: 2829 (29%) | 17.1 img/s | ETA: 2 min\n",
      "  9,800/11,669 | OK: 2929 (30%) | 16.7 img/s | ETA: 2 min\n",
      "  [checkpoint] 4,092 products in cache\n",
      "  9,900/11,669 | OK: 3029 (31%) | 16.4 img/s | ETA: 2 min\n",
      "  10,000/11,669 | OK: 3129 (31%) | 16.0 img/s | ETA: 2 min\n",
      "  [checkpoint] 4,292 products in cache\n",
      "  10,100/11,669 | OK: 3229 (32%) | 15.7 img/s | ETA: 2 min\n",
      "  10,200/11,669 | OK: 3329 (33%) | 15.4 img/s | ETA: 2 min\n",
      "  [checkpoint] 4,492 products in cache\n",
      "  10,300/11,669 | OK: 3429 (33%) | 15.2 img/s | ETA: 2 min\n",
      "  10,400/11,669 | OK: 3529 (34%) | 14.8 img/s | ETA: 1 min\n",
      "  [checkpoint] 4,692 products in cache\n",
      "  10,500/11,669 | OK: 3629 (35%) | 14.6 img/s | ETA: 1 min\n",
      "  10,600/11,669 | OK: 3729 (35%) | 14.4 img/s | ETA: 1 min\n",
      "  [checkpoint] 4,892 products in cache\n",
      "  10,700/11,669 | OK: 3829 (36%) | 14.1 img/s | ETA: 1 min\n",
      "  10,800/11,669 | OK: 3929 (36%) | 13.9 img/s | ETA: 1 min\n",
      "  [checkpoint] 5,092 products in cache\n",
      "  10,900/11,669 | OK: 4029 (37%) | 13.7 img/s | ETA: 1 min\n",
      "  11,000/11,669 | OK: 4129 (38%) | 13.5 img/s | ETA: 1 min\n",
      "  [checkpoint] 5,292 products in cache\n",
      "  11,100/11,669 | OK: 4229 (38%) | 13.3 img/s | ETA: 1 min\n",
      "  11,200/11,669 | OK: 4329 (39%) | 13.1 img/s | ETA: 1 min\n",
      "  [checkpoint] 5,491 products in cache\n",
      "  11,300/11,669 | OK: 4429 (39%) | 12.9 img/s | ETA: 0 min\n",
      "  [domain tracker] Skipping ui.assets-asda.com:443 (success rate 0% after 30 attempts)\n",
      "  11,400/11,669 | OK: 4457 (39%) | 12.5 img/s | ETA: 0 min\n",
      "  [checkpoint] 5,617 products in cache\n",
      "  11,500/11,669 | OK: 4457 (39%) | 12.6 img/s | ETA: 0 min\n",
      "  11,600/11,669 | OK: 4457 (38%) | 12.8 img/s | ETA: 0 min\n",
      "  [checkpoint] 5,617 products in cache\n",
      "\n",
      "STEP 6: Saving...\n",
      "  Cache: 5,617 products → /Users/leoss/Desktop/Portfolio/Website-/UK pink tax/Outputs/color_features_cache_v3_filtered.csv\n",
      "  Failed URLs log → /Users/leoss/Desktop/Portfolio/Website-/UK pink tax/Outputs/failed_urls.csv\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Processed: 11,669\n",
      "Extracted: 4,457 (38.2%)\n",
      "Failed:    7,212 (61.8%)\n",
      "\n",
      "Failure breakdown:\n",
      "  adaptive_skip: 6991 (59.9%)\n",
      "  head_check_dead: 171 (1.5%)\n",
      "  connection_error: 30 (0.3%)\n",
      "  http_404: 20 (0.2%)\n",
      "\n",
      "Per-domain results:\n",
      "  domain                                      ok / total    rate\n",
      "  ------------------------------------------------------------\n",
      "  digitalcontent.api.tesco.com                10 /  6880     0%\n",
      "  groceries.morrisons.com                   4447 /  4447   100%\n",
      "  ui.assets-asda.com:443                       0 /   342     0%\n",
      "\n",
      "Gender split in cache:\n",
      "  none: 4,746\n",
      "  female: 460\n",
      "  male: 411\n",
      "\n",
      "File size: 534.0 KB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COLOR EXTRACTION - REVISED WITH ADAPTIVE DOMAIN HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = Path('/Users/leoss')\n",
    "DATA_DIR = BASE_DIR / 'Downloads'\n",
    "OUTPUT_BASE = BASE_DIR / 'Desktop/Portfolio/Website-/UK pink tax/Outputs'\n",
    "\n",
    "PATH_MAIN_DATA = DATA_DIR / 'items_fin.csv'\n",
    "COLOR_CACHE_PATH = OUTPUT_BASE / 'color_features_cache_v3_filtered.csv'\n",
    "FAILED_URLS_PATH = OUTPUT_BASE / 'failed_urls.csv'\n",
    "\n",
    "# Extraction settings\n",
    "N_COLORS = 3\n",
    "TIMEOUT = 15\n",
    "MAX_SAMPLES = 20000\n",
    "PRIORITIZE_GENDERED = False\n",
    "MAX_SAMPLES = 20000  # already set, just confirming — 5,517 is well under this\n",
    "\n",
    "# Retry settings (only for transient errors)\n",
    "MAX_RETRIES = 1\n",
    "RETRY_DELAY = 1\n",
    "\n",
    "# Incremental save interval\n",
    "SAVE_EVERY = 200\n",
    "\n",
    "# Adaptive domain thresholds\n",
    "MIN_DOMAIN_SAMPLES = 30        # need this many attempts before judging a domain\n",
    "HEAD_CHECK_THRESHOLD = 0.40    # if success rate < 40%, use HEAD pre-checks\n",
    "SKIP_THRESHOLD = 0.05          # if success rate < 5%, skip entirely\n",
    "\n",
    "# Categories to exclude\n",
    "EXCLUDE_CATEGORIES = [\n",
    "    'food', 'grocery', 'groceries', 'snacks', 'drinks', 'beverages',\n",
    "    'pet food', 'pet supplies', 'cleaning', 'household', 'kitchen',\n",
    "    'office', 'stationery', 'electronics', 'tech', 'garden', 'automotive'\n",
    "]\n",
    "\n",
    "COL_PRODUCT_ID = 'product_id'\n",
    "COL_IMAGE = 'image_url'\n",
    "COL_BREADCRUMB = 'standardized_breadcrumbs'\n",
    "COL_NAME = 'product_title_x'\n",
    "COL_DESC = 'description'\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ============================================================================\n",
    "# COLOR DEFINITIONS\n",
    "# ============================================================================\n",
    "\n",
    "STANDARD_COLORS = {\n",
    "    'dark_red': (139, 0, 0), 'red': (255, 0, 0), 'coral': (255, 127, 80),\n",
    "    'salmon': (250, 128, 114), 'crimson': (220, 20, 60), 'brown': (139, 69, 19),\n",
    "    'tan': (210, 180, 140), 'orange': (255, 165, 0), 'gold': (255, 215, 0),\n",
    "    'yellow': (255, 255, 0), 'khaki': (240, 230, 140), 'dark_green': (0, 100, 0),\n",
    "    'green': (0, 128, 0), 'lime': (50, 205, 50), 'olive': (128, 128, 0),\n",
    "    'teal': (0, 128, 128), 'navy': (0, 0, 128), 'blue': (0, 0, 255),\n",
    "    'royal_blue': (65, 105, 225), 'sky_blue': (135, 206, 235), 'cyan': (0, 255, 255),\n",
    "    'purple': (128, 0, 128), 'magenta': (255, 0, 255), 'violet': (238, 130, 238),\n",
    "    'lavender': (230, 230, 250), 'pink': (255, 192, 203), 'hot_pink': (255, 105, 180),\n",
    "    'gray': (128, 128, 128), 'silver': (192, 192, 192),\n",
    "    'black': (0, 0, 0), 'white': (255, 255, 255),\n",
    "}\n",
    "\n",
    "FEMALE_KEYWORDS = [\n",
    "    'women', 'woman', 'female', 'ladies', 'lady', 'girls',\n",
    "    'womens', \"women's\", 'femme', 'her', 'feminine', 'fem',\n",
    "]\n",
    "MALE_KEYWORDS = [\n",
    "    'men', 'man', 'male', 'gentleman', 'gentlemen', 'boys',\n",
    "    'mens', \"men's\", 'homme', 'his', 'masculine',\n",
    "]\n",
    "\n",
    "# Shared session for connection pooling\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    'User-Agent': (\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '\n",
    "        'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "        'Chrome/91.0.4472.124 Safari/537.36'\n",
    "    ),\n",
    "    'Accept': 'image/avif,image/webp,image/apng,image/*,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "})\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ADAPTIVE DOMAIN TRACKER\n",
    "# ============================================================================\n",
    "\n",
    "class DomainTracker:\n",
    "    \"\"\"\n",
    "    Tracks per-domain success/failure rates during the run.\n",
    "    After MIN_DOMAIN_SAMPLES attempts, adjusts strategy:\n",
    "      - success rate < HEAD_CHECK_THRESHOLD → HEAD pre-check before GET\n",
    "      - success rate < SKIP_THRESHOLD → skip entirely\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_samples, head_threshold, skip_threshold):\n",
    "        self.min_samples = min_samples\n",
    "        self.head_threshold = head_threshold\n",
    "        self.skip_threshold = skip_threshold\n",
    "        self.attempts = Counter()   # domain → total attempts\n",
    "        self.successes = Counter()  # domain → successful extractions\n",
    "        self._notified_head = set()\n",
    "        self._notified_skip = set()\n",
    "\n",
    "    def record(self, domain, success):\n",
    "        self.attempts[domain] += 1\n",
    "        if success:\n",
    "            self.successes[domain] += 1\n",
    "\n",
    "    def success_rate(self, domain):\n",
    "        total = self.attempts[domain]\n",
    "        if total == 0:\n",
    "            return 1.0\n",
    "        return self.successes[domain] / total\n",
    "\n",
    "    def should_skip(self, domain):\n",
    "        if self.attempts[domain] < self.min_samples:\n",
    "            return False\n",
    "        skip = self.success_rate(domain) < self.skip_threshold\n",
    "        if skip and domain not in self._notified_skip:\n",
    "            rate = self.success_rate(domain)\n",
    "            print(f\"  [domain tracker] Skipping {domain} \"\n",
    "                  f\"(success rate {rate:.0%} after {self.attempts[domain]} attempts)\")\n",
    "            self._notified_skip.add(domain)\n",
    "        return skip\n",
    "\n",
    "    def should_head_check(self, domain):\n",
    "        if self.attempts[domain] < self.min_samples:\n",
    "            return False\n",
    "        rate = self.success_rate(domain)\n",
    "        head_check = rate < self.head_threshold and rate >= self.skip_threshold\n",
    "        if head_check and domain not in self._notified_head:\n",
    "            print(f\"  [domain tracker] HEAD pre-checking {domain} \"\n",
    "                  f\"(success rate {rate:.0%} after {self.attempts[domain]} attempts)\")\n",
    "            self._notified_head.add(domain)\n",
    "        return head_check\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"Return a summary dict of {domain: (successes, attempts, rate)}.\"\"\"\n",
    "        out = {}\n",
    "        for domain in sorted(self.attempts, key=lambda d: self.attempts[d], reverse=True):\n",
    "            total = self.attempts[domain]\n",
    "            ok = self.successes[domain]\n",
    "            out[domain] = (ok, total, ok / total if total else 0)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Global tracker instance (initialized in main)\n",
    "domain_tracker = None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_domain(url):\n",
    "    try:\n",
    "        return str(url).split('/')[2]\n",
    "    except (IndexError, AttributeError):\n",
    "        return 'unknown'\n",
    "\n",
    "\n",
    "def color_distance(c1, c2):\n",
    "    return np.sqrt(sum((a - b) ** 2 for a, b in zip(c1, c2)))\n",
    "\n",
    "\n",
    "def closest_standard_color(rgb):\n",
    "    min_dist = float('inf')\n",
    "    closest = 'gray'\n",
    "    for name, std_rgb in STANDARD_COLORS.items():\n",
    "        dist = color_distance(rgb, std_rgb)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest = name\n",
    "    return closest\n",
    "\n",
    "\n",
    "def is_background_color(rgb):\n",
    "    r, g, b = rgb\n",
    "    if r > 240 and g > 240 and b > 240:\n",
    "        return True\n",
    "    if r < 15 and g < 15 and b < 15:\n",
    "        return True\n",
    "    max_diff = max(abs(r - g), abs(g - b), abs(r - b))\n",
    "    avg = (r + g + b) / 3\n",
    "    if max_diff < 20 and 100 < avg < 160:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def head_check_alive(url, timeout=5):\n",
    "    \"\"\"Quick HEAD request to check if URL exists before full download.\"\"\"\n",
    "    try:\n",
    "        resp = SESSION.head(url, timeout=timeout, allow_redirects=True)\n",
    "        return resp.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_transient_error(status_code):\n",
    "    \"\"\"Whether an HTTP error code is worth retrying (server-side / rate limit).\"\"\"\n",
    "    if status_code is None:\n",
    "        return True\n",
    "    return status_code >= 500 or status_code == 429\n",
    "\n",
    "\n",
    "def extract_colors(image_url, n_colors=3, timeout=15, max_retries=1):\n",
    "    \"\"\"\n",
    "    Extract dominant colors from a product image.\n",
    "    Uses domain_tracker to decide whether to HEAD-check or skip.\n",
    "\n",
    "    Returns (colors_list | None, error_reason | None).\n",
    "    \"\"\"\n",
    "    url = str(image_url).strip()\n",
    "    domain = get_domain(url)\n",
    "\n",
    "    # --- Adaptive domain handling ---\n",
    "    if domain_tracker.should_skip(domain):\n",
    "        return None, 'adaptive_skip'\n",
    "\n",
    "    if domain_tracker.should_head_check(domain):\n",
    "        if not head_check_alive(url, timeout=5):\n",
    "            return None, 'head_check_dead'\n",
    "\n",
    "    # --- Download with selective retries ---\n",
    "    last_error = None\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            response = SESSION.get(url, timeout=timeout, allow_redirects=True)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                last_error = f'http_{response.status_code}'\n",
    "                # Only retry transient server errors, not 404/403\n",
    "                if is_transient_error(response.status_code) and attempt < max_retries:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                    continue\n",
    "                return None, last_error\n",
    "\n",
    "            # --- Image processing ---\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            img = img.resize((100, 100))\n",
    "\n",
    "            pixels = np.array(img).reshape(-1, 3)\n",
    "\n",
    "            non_bg = np.array([p for p in pixels if not is_background_color(tuple(p))])\n",
    "            if len(non_bg) < 50:\n",
    "                non_bg = pixels\n",
    "\n",
    "            n_clusters = min(n_colors + 2, len(non_bg))\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            kmeans.fit(non_bg)\n",
    "\n",
    "            counts = Counter(kmeans.labels_)\n",
    "            total = len(kmeans.labels_)\n",
    "\n",
    "            colors = []\n",
    "            for cluster_id, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                rgb = tuple(int(c) for c in kmeans.cluster_centers_[cluster_id])\n",
    "                if not is_background_color(rgb):\n",
    "                    colors.append({'name': closest_standard_color(rgb), 'weight': count / total})\n",
    "                if len(colors) >= n_colors:\n",
    "                    break\n",
    "\n",
    "            return (colors, None) if colors else (None, 'no_non_bg_colors')\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            last_error = 'timeout'\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            last_error = 'connection_error'\n",
    "        except requests.exceptions.RequestException:\n",
    "            last_error = 'request_error'\n",
    "        except Exception:\n",
    "            last_error = 'processing_error'\n",
    "\n",
    "        if attempt < max_retries:\n",
    "            time.sleep(RETRY_DELAY)\n",
    "\n",
    "    return None, last_error\n",
    "\n",
    "\n",
    "def contains_excluded_category(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text_lower = str(text).lower()\n",
    "    return any(cat in text_lower for cat in EXCLUDE_CATEGORIES)\n",
    "\n",
    "\n",
    "def extract_gender_label(row):\n",
    "    def check_gender(text):\n",
    "        if pd.isna(text) or str(text).strip() == '':\n",
    "            return 'none'\n",
    "        text_lower = str(text).lower()\n",
    "        has_female = any(re.search(r'\\b' + kw + r'\\b', text_lower) for kw in FEMALE_KEYWORDS)\n",
    "        has_male = any(re.search(r'\\b' + kw + r'\\b', text_lower) for kw in MALE_KEYWORDS)\n",
    "        if has_female and not has_male:\n",
    "            return 'female'\n",
    "        elif has_male and not has_female:\n",
    "            return 'male'\n",
    "        elif has_female and has_male:\n",
    "            return 'both'\n",
    "        return 'none'\n",
    "\n",
    "    for col in [COL_BREADCRUMB, COL_NAME, COL_DESC]:\n",
    "        if col in row.index:\n",
    "            gender = check_gender(row[col])\n",
    "            if gender in ['female', 'male']:\n",
    "                return gender\n",
    "    return 'none'\n",
    "\n",
    "\n",
    "def save_incremental(new_results, cache_path):\n",
    "    \"\"\"Append new results to the cache CSV, deduplicating on product_id.\"\"\"\n",
    "    if not new_results:\n",
    "        return 0\n",
    "\n",
    "    new_df = pd.DataFrame(new_results)\n",
    "\n",
    "    if cache_path.exists():\n",
    "        existing = pd.read_csv(cache_path)\n",
    "        combined = pd.concat([existing, new_df]).drop_duplicates(subset=[COL_PRODUCT_ID])\n",
    "    else:\n",
    "        combined = new_df\n",
    "\n",
    "    combined.to_csv(cache_path, index=False)\n",
    "    return len(combined)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def extract_and_save_colors():\n",
    "    global domain_tracker\n",
    "    domain_tracker = DomainTracker(\n",
    "        min_samples=MIN_DOMAIN_SAMPLES,\n",
    "        head_threshold=HEAD_CHECK_THRESHOLD,\n",
    "        skip_threshold=SKIP_THRESHOLD,\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COLOR EXTRACTION  (adaptive domain handling)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Timeout: {TIMEOUT}s | Retries: {MAX_RETRIES} (transient only)\")\n",
    "    print(f\"Adaptive thresholds: HEAD-check < {HEAD_CHECK_THRESHOLD:.0%} \"\n",
    "          f\"success, skip < {SKIP_THRESHOLD:.0%} success \"\n",
    "          f\"(after {MIN_DOMAIN_SAMPLES} samples)\")\n",
    "    print()\n",
    "\n",
    "    OUTPUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Resume support ----\n",
    "    already_done = set()\n",
    "    if COLOR_CACHE_PATH.exists():\n",
    "        existing = pd.read_csv(COLOR_CACHE_PATH)\n",
    "        if COL_PRODUCT_ID in existing.columns:\n",
    "            already_done = set(existing[COL_PRODUCT_ID].astype(str))\n",
    "            print(f\"Existing cache: {len(already_done):,} products — will resume.\\n\")\n",
    "\n",
    "    # ---- Load & filter ----\n",
    "    print(\"STEP 1: Loading data...\")\n",
    "    df = pd.read_csv(PATH_MAIN_DATA, encoding='latin-1')\n",
    "    df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "    if 'unnamed:_0' in df.columns:\n",
    "        df = df.drop(columns=['unnamed:_0'])\n",
    "    print(f\"  Loaded {len(df):,} products\")\n",
    "\n",
    "    print(\"\\nSTEP 2: Filtering categories...\")\n",
    "    df['is_excluded'] = df[COL_BREADCRUMB].apply(contains_excluded_category)\n",
    "    df = df[~df['is_excluded']].copy().reset_index(drop=True)\n",
    "    print(f\"  Remaining: {len(df):,} products\")\n",
    "\n",
    "    print(\"\\nSTEP 3: Gender labels...\")\n",
    "    df['label_extracted'] = df.apply(extract_gender_label, axis=1)\n",
    "    for label, count in df['label_extracted'].value_counts().items():\n",
    "        print(f\"  {label}: {count:,}\")\n",
    "\n",
    "    print(\"\\nSTEP 4: Selecting products...\")\n",
    "    df_with_images = df[df[COL_IMAGE].notna()].copy()\n",
    "\n",
    "    # Show domain breakdown before any filtering\n",
    "    df_with_images['_domain'] = df_with_images[COL_IMAGE].apply(get_domain)\n",
    "    print(\"  Domain breakdown:\")\n",
    "    for domain, count in df_with_images['_domain'].value_counts().items():\n",
    "        print(f\"    {domain}: {count:,}\")\n",
    "\n",
    "    if PRIORITIZE_GENDERED:\n",
    "        gendered = df_with_images[df_with_images['label_extracted'].isin(['female', 'male'])].copy()\n",
    "        none_products = df_with_images[df_with_images['label_extracted'] == 'none']\n",
    "        none_sample_size = min(NONE_SAMPLE_SIZE, len(none_products))\n",
    "        none_sample = (\n",
    "            none_products.sample(n=none_sample_size, random_state=RANDOM_STATE)\n",
    "            if none_sample_size > 0 else pd.DataFrame()\n",
    "        )\n",
    "        to_extract = pd.concat([gendered, none_sample]).drop_duplicates()\n",
    "        print(f\"  Gendered: {len(gendered):,} | None sample: {len(none_sample):,}\")\n",
    "    else:\n",
    "        to_extract = df_with_images.copy()\n",
    "\n",
    "    if len(to_extract) > MAX_SAMPLES:\n",
    "        to_extract = to_extract.sample(n=MAX_SAMPLES, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Skip already-processed\n",
    "    to_extract = to_extract[\n",
    "        ~to_extract[COL_PRODUCT_ID].astype(str).isin(already_done)\n",
    "    ]\n",
    "    print(f\"  To process (after resume filter): {len(to_extract):,}\")\n",
    "\n",
    "    # ---- Extract ----\n",
    "    print(f\"\\nSTEP 5: Extracting colors (top {N_COLORS} per image)...\")\n",
    "    print()\n",
    "\n",
    "    color_results = []\n",
    "    failed_records = []\n",
    "    error_counter = Counter()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (row_idx, row) in enumerate(to_extract.iterrows()):\n",
    "        if (idx + 1) % 100 == 0 or idx == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (idx + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (len(to_extract) - idx - 1) / rate if rate > 0 else 0\n",
    "            ok = len(color_results)\n",
    "            total_so_far = idx + 1\n",
    "            pct = 100 * ok / total_so_far if total_so_far else 0\n",
    "            print(\n",
    "                f\"  {idx+1:,}/{len(to_extract):,} \"\n",
    "                f\"| OK: {ok} ({pct:.0f}%) \"\n",
    "                f\"| {rate:.1f} img/s \"\n",
    "                f\"| ETA: {remaining/60:.0f} min\"\n",
    "            )\n",
    "\n",
    "        url = row[COL_IMAGE]\n",
    "        domain = get_domain(str(url))\n",
    "\n",
    "        colors, error = extract_colors(\n",
    "            url, n_colors=N_COLORS, timeout=TIMEOUT, max_retries=MAX_RETRIES\n",
    "        )\n",
    "\n",
    "        success = colors is not None\n",
    "        domain_tracker.record(domain, success)\n",
    "\n",
    "        if success:\n",
    "            entry = {\n",
    "                'original_index': row_idx,\n",
    "                COL_PRODUCT_ID: row[COL_PRODUCT_ID],\n",
    "                'label_extracted': row['label_extracted'],\n",
    "            }\n",
    "            for i, c in enumerate(colors):\n",
    "                entry[f'color{i+1}_name'] = c['name']\n",
    "                entry[f'color{i+1}_weight'] = c['weight']\n",
    "            color_results.append(entry)\n",
    "        else:\n",
    "            error_counter[error] += 1\n",
    "            failed_records.append({\n",
    "                'product_id': row[COL_PRODUCT_ID],\n",
    "                'url': url,\n",
    "                'label': row['label_extracted'],\n",
    "                'error': error,\n",
    "            })\n",
    "\n",
    "        # Incremental save\n",
    "        if (idx + 1) % SAVE_EVERY == 0 and color_results:\n",
    "            total_in_cache = save_incremental(color_results, COLOR_CACHE_PATH)\n",
    "            print(f\"  [checkpoint] {total_in_cache:,} products in cache\")\n",
    "\n",
    "    # ---- Final save ----\n",
    "    print(f\"\\nSTEP 6: Saving...\")\n",
    "    if color_results:\n",
    "        total_in_cache = save_incremental(color_results, COLOR_CACHE_PATH)\n",
    "        print(f\"  Cache: {total_in_cache:,} products → {COLOR_CACHE_PATH}\")\n",
    "\n",
    "    if failed_records:\n",
    "        pd.DataFrame(failed_records).to_csv(FAILED_URLS_PATH, index=False)\n",
    "        print(f\"  Failed URLs log → {FAILED_URLS_PATH}\")\n",
    "\n",
    "    # ---- Summary ----\n",
    "    total = len(to_extract)\n",
    "    success_count = len(color_results)\n",
    "    success_rate = 100 * success_count / total if total else 0\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Processed: {total:,}\")\n",
    "    print(f\"Extracted: {success_count:,} ({success_rate:.1f}%)\")\n",
    "    print(f\"Failed:    {total - success_count:,} ({100 - success_rate:.1f}%)\")\n",
    "\n",
    "    print(\"\\nFailure breakdown:\")\n",
    "    for error, count in error_counter.most_common(10):\n",
    "        print(f\"  {error}: {count} ({100*count/total:.1f}%)\")\n",
    "\n",
    "    print(\"\\nPer-domain results:\")\n",
    "    print(f\"  {'domain':<40s} {'ok':>5s} / {'total':>5s}  {'rate':>6s}\")\n",
    "    print(f\"  {'-'*60}\")\n",
    "    for domain, (ok, tot, rate) in domain_tracker.summary().items():\n",
    "        print(f\"  {domain:<40s} {ok:>5d} / {tot:>5d}  {rate:>5.0%}\")\n",
    "\n",
    "    if success_count:\n",
    "        color_df = pd.read_csv(COLOR_CACHE_PATH)\n",
    "        if 'label_extracted' in color_df.columns:\n",
    "            print(f\"\\nGender split in cache:\")\n",
    "            for label, count in color_df['label_extracted'].value_counts().items():\n",
    "                print(f\"  {label}: {count:,}\")\n",
    "        print(f\"\\nFile size: {COLOR_CACHE_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        extract_and_save_colors()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nInterrupted. Partial results saved if any checkpoints were reached.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9abb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n",
      "✓ Output directory: /Users/leoss/Desktop/Portfolio/Website-/UK pink tax/Outputs/charts/ml_pipeline_v4\n",
      "\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "✓ Main dataset: 21,436 products\n",
      "✓ Human-coded: 259 products\n",
      "✓ Your labeled: 44 products\n",
      "\n",
      "======================================================================\n",
      "STEP 1: FILTER NON-GENDERED CATEGORIES\n",
      "======================================================================\n",
      "Original: 21,436\n",
      "Excluded: 8,604\n",
      "Remaining: 12,832\n",
      "\n",
      "======================================================================\n",
      "STEP 2: EXTRACT GENDER LABELS\n",
      "======================================================================\n",
      "Label distribution:\n",
      "{'none': 10913, 'female': 1075, 'male': 844}\n",
      "Human labels merged: 200\n",
      "\n",
      "======================================================================\n",
      "STEP 3: LOAD COLOR DATA\n",
      "======================================================================\n",
      "✓ Loaded color cache: 5,617 products\n",
      "  Gender split: {'none': 4746, 'female': 460, 'male': 411}\n",
      "  Matched to filtered data: 5,617 / 5,617\n",
      "\n",
      "======================================================================\n",
      "STEP 4: PREPARE TRAINING DATA\n",
      "======================================================================\n",
      "Explicitly female: 1075\n",
      "Explicitly male: 844\n",
      "\n",
      "None class sources:\n",
      "  Human-coded none: 48\n",
      "  Extracted none (unlabeled): 10849\n",
      "  Final none class: 844\n",
      "\n",
      "Balancing to: 844 per class\n",
      "\n",
      "✓ Training data: 2532 products\n",
      "  Class distribution: {0: 850, 1: 845, 2: 837}\n",
      "  (0=female, 1=male, 2=none)\n",
      "\n",
      "======================================================================\n",
      "STEP 5: TRAIN-TEST SPLIT\n",
      "======================================================================\n",
      "Train: 1905\n",
      "Test: 639\n",
      "Train distribution: {0: 643, 1: 634, 2: 628}\n",
      "\n",
      "======================================================================\n",
      "STEP 6: FEATURE ENGINEERING\n",
      "======================================================================\n",
      "✓ Price features: 2\n",
      "✓ Store features: 4\n",
      "✓ Breadcrumb TF-IDF: 200 features\n",
      "✓ Description TF-IDF: 500 features\n",
      "  Color lookup built: 5,617 products\n",
      "  Color features filled for 840/1905 rows\n",
      "  Color features filled for 300/639 rows\n",
      "✓ Color features: 93 (available for 840/1905 train samples)\n",
      "\n",
      "======================================================================\n",
      "STEP 7: BUILD FEATURE MATRICES\n",
      "======================================================================\n",
      "X_train: (1905, 799)\n",
      "X_test: (639, 799)\n",
      "Features: 799\n",
      "✓ Feature alignment validated\n",
      "\n",
      "======================================================================\n",
      "STEP 8: TRAIN MODELS\n",
      "======================================================================\n",
      "\n",
      "--- Logistic Regression (L1) ---\n",
      "Accuracy: 0.6808, F1: 0.6793\n",
      "\n",
      "--- Logistic Regression (L2) ---\n",
      "Accuracy: 0.7574, F1: 0.7581\n",
      "\n",
      "--- Random Forest ---\n",
      "Accuracy: 0.7433, F1: 0.7424\n",
      "\n",
      "--- Histogram Gradient Boosting ---\n",
      "Accuracy: 0.8028, F1: 0.8038\n",
      "\n",
      "--- SVM (RBF) ---\n",
      "Accuracy: 0.7653, F1: 0.7646\n",
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n",
      "                 Model  Accuracy  F1_weighted\n",
      "Hist Gradient Boosting  0.802817     0.803755\n",
      "                   SVM  0.765258     0.764591\n",
      "            L2 (Ridge)  0.757433     0.758084\n",
      "         Random Forest  0.743349     0.742396\n",
      "            L1 (LASSO)  0.680751     0.679254\n",
      "\n",
      "======================================================================\n",
      "BEST MODEL ANALYSIS\n",
      "======================================================================\n",
      "Best model: Hist Gradient Boosting\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.80      0.79      0.80       217\n",
      "        male       0.89      0.78      0.83       213\n",
      "        none       0.74      0.84      0.78       209\n",
      "\n",
      "    accuracy                           0.80       639\n",
      "   macro avg       0.81      0.80      0.80       639\n",
      "weighted avg       0.81      0.80      0.80       639\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "            Predicted\n",
      "            female  male  none\n",
      "Actual female   172    11    34\n",
      "Actual male      19   166    28\n",
      "Actual none      24    10   175\n",
      "\n",
      "======================================================================\n",
      "FEATURE IMPORTANCE (L1 Model)\n",
      "======================================================================\n",
      "\n",
      "Top 15 FEMALE features:\n",
      "  bc_period                               : +0.7407\n",
      "  bc_shaving hair                         : +0.7071\n",
      "  bc_intimate                             : +0.6936\n",
      "  bc_bc period                            : +0.6890\n",
      "  bc_removal                              : +0.6578\n",
      "  bc_hair removal                         : +0.6578\n",
      "  bc_removal bc                           : +0.5919\n",
      "  desc_venus                              : +0.5633\n",
      "  bc_products                             : +0.5188\n",
      "  bc_period products                      : +0.5188\n",
      "  desc_leaks                              : +0.4431\n",
      "  desc_protection                         : +0.4259\n",
      "  bc_intimate care                        : +0.4223\n",
      "  desc_normal                             : +0.4166\n",
      "  bc_products intimate                    : +0.3873\n",
      "\n",
      "Top 15 MALE features:\n",
      "  bc_toiletries bc                        : +1.3467\n",
      "  bc_bc toiletries                        : +1.2384\n",
      "  bc_toiletries                           : +1.1913\n",
      "  desc_lynx                               : +0.9798\n",
      "  bc_gel                                  : +0.6771\n",
      "  feat_color1_black                       : +0.6257\n",
      "  bc_razors                               : +0.5879\n",
      "  bc_bc razors                            : +0.5879\n",
      "  desc_gillette                           : +0.5395\n",
      "  bc_blades                               : +0.5222\n",
      "  bc_bc skincare                          : +0.5161\n",
      "  desc_face                               : +0.5130\n",
      "  desc_shave                              : +0.4891\n",
      "  bc_deodorants                           : +0.4601\n",
      "  bc_bc deodorants                        : +0.4516\n",
      "\n",
      "Top 15 NONE features:\n",
      "  bc_toothpaste                           : +0.6384\n",
      "  bc_accessories                          : +0.6161\n",
      "  bc_conditioner                          : +0.6072\n",
      "  bc_marketplace bc                       : +0.5011\n",
      "  bc_marketplace                          : +0.5011\n",
      "  bc_bc marketplace                       : +0.5011\n",
      "  bc_accessories bc                       : +0.4531\n",
      "  bc_dine bc                              : +0.3971\n",
      "  bc_cook dine                            : +0.3971\n",
      "  bc_cook                                 : +0.3971\n",
      "  bc_dine                                 : +0.3971\n",
      "  bc_bc cook                              : +0.3971\n",
      "  bc_care                                 : +0.3877\n",
      "  bc_bc frozen                            : +0.3854\n",
      "  bc_frozen                               : +0.3854\n",
      "\n",
      "======================================================================\n",
      "STEP 11: PREDICT ON ALL PRODUCTS\n",
      "======================================================================\n",
      "Full dataset: (12832, 799)\n",
      "\n",
      "Prediction distribution:\n",
      "ml_pred_label\n",
      "none      9407\n",
      "female    2252\n",
      "male      1173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "STEP 12: COLOR FEATURE IMPACT ANALYSIS\n",
      "======================================================================\n",
      "WITH colors:    Accuracy=0.6808, F1=0.6793\n",
      "WITHOUT colors: Accuracy=0.6604, F1=0.6567\n",
      "Color impact:   Accuracy +2.03pp, F1 +2.25pp\n",
      "\n",
      "On color-available subset (300 test samples):\n",
      "  WITH colors:    Accuracy=0.7167\n",
      "  WITHOUT colors: Accuracy=0.6733\n",
      "\n",
      "======================================================================\n",
      "STEP 13: VALIDATION VS HUMAN LABELS\n",
      "======================================================================\n",
      "Products with human labels: 200\n",
      "Accuracy vs human: 0.7200\n",
      "\n",
      "Confusion (ML vs Human):\n",
      "            ML Predicted\n",
      "            female  male  none\n",
      "Human female    55     9    16\n",
      "Human male      16    51     5\n",
      "Human none       7     3    38\n",
      "\n",
      "======================================================================\n",
      "STEP 14: IMPLICIT GENDERING\n",
      "======================================================================\n",
      "Implicit female (>50% conf): 492\n",
      "Implicit male (>50% conf): 197\n",
      "Predicted none: 9,407\n",
      "\n",
      "Sample implicit FEMALE:\n",
      "  L'oreal Paris Age Perfect Refreshing Toner 200Ml... (conf: 0.52)\n",
      "  Cute 9th Birthday Card... (conf: 0.54)\n",
      "  60th Birthday Card - Ruby... (conf: 0.52)\n",
      "  Funny Baking Birthday Card For Mum... (conf: 0.57)\n",
      "  Bassetts Multivitamin Gummies,  3-6 years Strawberry x 30... (conf: 0.63)\n",
      "\n",
      "Sample implicit MALE:\n",
      "  Original Source Spiced Lime 3 In 1 Bodywash 500Ml... (conf: 0.55)\n",
      "  Radox Grapefruit & Ginger Feel Uplifted Shower Gel Body Wash... (conf: 0.58)\n",
      "  Gillette Fusion 5 Shave Gel Sensitive 75Ml... (conf: 0.52)\n",
      "  Simple Refreshing Shower Gel Body Wash 225ml... (conf: 0.56)\n",
      "  Sanex Expert Skin Health Sensitive Shower Gel Body Wash 450M... (conf: 0.52)\n",
      "\n",
      "======================================================================\n",
      "STEP 15: EXPORT VALIDATION SAMPLE\n",
      "======================================================================\n",
      "Available for validation: 12,632\n",
      "  Sampled 85 female\n",
      "  Sampled 85 male\n",
      "  Sampled 85 none\n",
      "\n",
      "✓ Saved: /Users/leoss/Desktop/Portfolio/Website-/UK pink tax/Outputs/charts/ml_pipeline_v4/validation_sample.csv (255 products)\n",
      "\n",
      "======================================================================\n",
      "PIPELINE SUMMARY (v4)\n",
      "======================================================================\n",
      "\n",
      "DATA:\n",
      "  Original: 21,436\n",
      "  After filtering: 12,832\n",
      "  Excluded categories: 8,604\n",
      "\n",
      "TRAINING:\n",
      "  Total samples: 2,532 (balanced 3-class)\n",
      "  Train: 1,905\n",
      "  Test: 639\n",
      "  Products with colors: 5,617 (Morrisons only — Tesco/ASDA CDN links expired)\n",
      "\n",
      "BEST MODEL: Hist Gradient Boosting (F1: 0.8038)\n",
      "\n",
      "COLOR IMPACT:\n",
      "  With colors:    F1=0.6793\n",
      "  Without colors: F1=0.6567\n",
      "\n",
      "PREDICTIONS:\n",
      "  Female: 2,252\n",
      "  Male: 1,173\n",
      "  None: 9,407\n",
      "\n",
      "IMPLICIT GENDERING:\n",
      "  Implicit female: 492\n",
      "  Implicit male: 197\n",
      "\n",
      "OUTPUT:\n",
      "  /Users/leoss/Desktop/Portfolio/Website-/UK pink tax/Outputs/charts/ml_pipeline_v4/\n",
      "  ├── model_comparison.csv\n",
      "  ├── feature_importance.csv\n",
      "  └── validation_sample.csv\n",
      "\n",
      "NOTE: Color features are limited to Morrisons products (~5,600 of 12,800).\n",
      "      Tesco and ASDA image CDN links are expired. Color-based findings\n",
      "      should be interpreted with this coverage limitation in mind.\n",
      "\n",
      "✓ Pipeline complete\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PINK TAX ANALYSIS: ML-BASED GENDER PREDICTION PIPELINE v4\n",
    "# ============================================================================\n",
    "#\n",
    "# Changes from v3:\n",
    "#   - Color extraction removed (uses pre-built cache: 5,617 products)\n",
    "#   - Color features merged on product_id, not index (fixes alignment bug\n",
    "#     caused by reset_index after category filtering)\n",
    "#   - Morrisons-only color data noted as limitation\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path('/Users/leoss')\n",
    "DATA_DIR = BASE_DIR / 'Downloads'\n",
    "OUTPUT_BASE = BASE_DIR / 'Desktop/Portfolio/Website-/UK pink tax/Outputs'\n",
    "\n",
    "PATH_MAIN_DATA = DATA_DIR / 'items_fin.csv'\n",
    "PATH_HUMAN_CODED = DATA_DIR / 'items_prices_description_gender_humancode_sample.csv'\n",
    "PATH_YOUR_LABELED = DATA_DIR / 'available_validation.xlsx'\n",
    "OUTPUT_DIR = OUTPUT_BASE / 'charts/ml_pipeline_v4'\n",
    "\n",
    "# Pre-built color cache (5,617 products, Morrisons only)\n",
    "COLOR_CACHE_PATH = OUTPUT_BASE / 'color_features_cache_v3_filtered.csv'\n",
    "\n",
    "# ML settings\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.25\n",
    "CV_FOLDS = 5\n",
    "N_COLORS = 3\n",
    "\n",
    "# Categories to exclude\n",
    "EXCLUDE_CATEGORIES = [\n",
    "    'food', 'grocery', 'groceries', 'snacks', 'drinks', 'beverages',\n",
    "    'pet food', 'pet supplies', 'cleaning', 'household', 'kitchen',\n",
    "    'office', 'stationery', 'electronics', 'tech', 'garden', 'automotive'\n",
    "]\n",
    "\n",
    "# Edge case thresholds\n",
    "MIN_CLASS_SIZE = 50\n",
    "MIN_TEST_SAMPLES = 10\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, f1_score,\n",
    "                             confusion_matrix, classification_report)\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports complete\")\n",
    "print(f\"✓ Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COLUMN NAMES\n",
    "# ============================================================================\n",
    "\n",
    "COL_NAME = 'product_title_x'\n",
    "COL_DESC = 'description'\n",
    "COL_BREADCRUMB = 'standardized_breadcrumbs'\n",
    "COL_PRICE = 'price'\n",
    "COL_UNIT_PRICE = 'unit_price'\n",
    "COL_IMAGE = 'image_url'\n",
    "COL_STORE = 'store_id'\n",
    "COL_PRODUCT_ID = 'product_id'\n",
    "COL_URL = 'product_url_x'\n",
    "\n",
    "FEMALE_KEYWORDS = ['women', 'woman', 'female', 'ladies', 'lady', 'girls',\n",
    "                   'womens', \"women's\", 'femme', 'her', 'feminine', 'fem']\n",
    "MALE_KEYWORDS = ['men', 'man', 'male', 'gentleman', 'gentlemen', 'boys',\n",
    "                 'mens', \"men's\", 'homme', 'his', 'masculine']\n",
    "ALL_GENDER_KEYWORDS = set(FEMALE_KEYWORDS + MALE_KEYWORDS)\n",
    "\n",
    "STANDARD_COLORS = {\n",
    "    'dark_red': (139, 0, 0), 'red': (255, 0, 0), 'coral': (255, 127, 80),\n",
    "    'salmon': (250, 128, 114), 'crimson': (220, 20, 60), 'brown': (139, 69, 19),\n",
    "    'tan': (210, 180, 140), 'orange': (255, 165, 0), 'gold': (255, 215, 0),\n",
    "    'yellow': (255, 255, 0), 'khaki': (240, 230, 140), 'dark_green': (0, 100, 0),\n",
    "    'green': (0, 128, 0), 'lime': (50, 205, 50), 'olive': (128, 128, 0),\n",
    "    'teal': (0, 128, 128), 'navy': (0, 0, 128), 'blue': (0, 0, 255),\n",
    "    'royal_blue': (65, 105, 225), 'sky_blue': (135, 206, 235), 'cyan': (0, 255, 255),\n",
    "    'purple': (128, 0, 128), 'magenta': (255, 0, 255), 'violet': (238, 130, 238),\n",
    "    'lavender': (230, 230, 250), 'pink': (255, 192, 203), 'hot_pink': (255, 105, 180),\n",
    "    'gray': (128, 128, 128), 'silver': (192, 192, 192),\n",
    "    'black': (0, 0, 0), 'white': (255, 255, 255),\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not PATH_MAIN_DATA.exists():\n",
    "    raise FileNotFoundError(f\"Main data file not found: {PATH_MAIN_DATA}\")\n",
    "\n",
    "df = pd.read_csv(PATH_MAIN_DATA, encoding='latin-1')\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "if 'unnamed:_0' in df.columns:\n",
    "    df = df.drop(columns=['unnamed:_0'])\n",
    "print(f\"✓ Main dataset: {len(df):,} products\")\n",
    "\n",
    "if not PATH_HUMAN_CODED.exists():\n",
    "    raise FileNotFoundError(f\"Human-coded file not found: {PATH_HUMAN_CODED}\")\n",
    "\n",
    "human_coded = pd.read_csv(PATH_HUMAN_CODED, encoding='latin-1')\n",
    "human_coded.columns = human_coded.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "if 'unnamed:_0' in human_coded.columns:\n",
    "    human_coded = human_coded.drop(columns=['unnamed:_0'])\n",
    "print(f\"✓ Human-coded: {len(human_coded)} products\")\n",
    "\n",
    "try:\n",
    "    your_labeled = pd.read_excel(PATH_YOUR_LABELED)\n",
    "    your_labeled.columns = your_labeled.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "    print(f\"✓ Your labeled: {len(your_labeled)} products\")\n",
    "except FileNotFoundError:\n",
    "    your_labeled = pd.DataFrame()\n",
    "    print(\"⚠ Your labeled file not found (optional)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: FILTER CATEGORIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: FILTER NON-GENDERED CATEGORIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "original_count = len(df)\n",
    "\n",
    "def contains_excluded_category(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text_lower = str(text).lower()\n",
    "    return any(cat in text_lower for cat in EXCLUDE_CATEGORIES)\n",
    "\n",
    "df['is_excluded'] = df[COL_BREADCRUMB].apply(contains_excluded_category)\n",
    "excluded_count = df['is_excluded'].sum()\n",
    "\n",
    "print(f\"Original: {original_count:,}\")\n",
    "print(f\"Excluded: {excluded_count:,}\")\n",
    "\n",
    "df = df[~df['is_excluded']].copy().reset_index(drop=True)\n",
    "print(f\"Remaining: {len(df):,}\")\n",
    "\n",
    "if len(df) == 0:\n",
    "    raise ValueError(\"No products remaining after filtering.\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: EXTRACT GENDER LABELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: EXTRACT GENDER LABELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def extract_gender_explicit(text):\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return 'none'\n",
    "    text_lower = str(text).lower()\n",
    "    has_female = any(re.search(r'\\b' + kw + r'\\b', text_lower) for kw in FEMALE_KEYWORDS)\n",
    "    has_male = any(re.search(r'\\b' + kw + r'\\b', text_lower) for kw in MALE_KEYWORDS)\n",
    "    if has_female and not has_male:\n",
    "        return 'female'\n",
    "    elif has_male and not has_female:\n",
    "        return 'male'\n",
    "    elif has_female and has_male:\n",
    "        return 'both'\n",
    "    return 'none'\n",
    "\n",
    "df['label_bc'] = df[COL_BREADCRUMB].apply(extract_gender_explicit)\n",
    "df['label_name'] = df[COL_NAME].apply(extract_gender_explicit)\n",
    "df['label_desc'] = df[COL_DESC].apply(extract_gender_explicit)\n",
    "\n",
    "def combine_labels(row):\n",
    "    for col in ['label_bc', 'label_name', 'label_desc']:\n",
    "        if row[col] in ['female', 'male']:\n",
    "            return row[col]\n",
    "    return 'none'\n",
    "\n",
    "df['label_extracted'] = df.apply(combine_labels, axis=1)\n",
    "\n",
    "print(f\"Label distribution:\")\n",
    "print(df['label_extracted'].value_counts().to_dict())\n",
    "\n",
    "# Merge human labels\n",
    "if 'human_gender_label' in human_coded.columns and COL_PRODUCT_ID in human_coded.columns:\n",
    "    human_labels = human_coded[[COL_PRODUCT_ID, 'human_gender_label']].drop_duplicates()\n",
    "    human_labels.columns = [COL_PRODUCT_ID, 'label_human']\n",
    "    human_labels['label_human'] = human_labels['label_human'].str.lower().str.strip()\n",
    "    df = df.merge(human_labels, on=COL_PRODUCT_ID, how='left')\n",
    "else:\n",
    "    df['label_human'] = None\n",
    "\n",
    "human_count = df['label_human'].notna().sum()\n",
    "print(f\"Human labels merged: {human_count}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: LOAD COLOR CACHE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: LOAD COLOR DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if COLOR_CACHE_PATH.exists():\n",
    "    color_df = pd.read_csv(COLOR_CACHE_PATH)\n",
    "    print(f\"✓ Loaded color cache: {len(color_df):,} products\")\n",
    "    if 'label_extracted' in color_df.columns:\n",
    "        print(f\"  Gender split: {color_df['label_extracted'].value_counts().to_dict()}\")\n",
    "\n",
    "    # Check how many match the filtered dataset\n",
    "    matched = color_df[COL_PRODUCT_ID].isin(df[COL_PRODUCT_ID]).sum()\n",
    "    print(f\"  Matched to filtered data: {matched:,} / {len(color_df):,}\")\n",
    "else:\n",
    "    print(\"⚠ No color cache found — proceeding without color features\")\n",
    "    color_df = pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: PREPARE TRAINING DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: PREPARE TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "female_all = df[df['label_extracted'] == 'female'].copy()\n",
    "male_all = df[df['label_extracted'] == 'male'].copy()\n",
    "\n",
    "print(f\"Explicitly female: {len(female_all)}\")\n",
    "print(f\"Explicitly male: {len(male_all)}\")\n",
    "\n",
    "if len(female_all) < MIN_CLASS_SIZE or len(male_all) < MIN_CLASS_SIZE:\n",
    "    raise ValueError(f\"Insufficient gendered samples. Need at least {MIN_CLASS_SIZE} per class.\")\n",
    "\n",
    "# None class: human-coded none + sample from extracted none\n",
    "human_none = df[(df['label_human'] == 'none')].copy()\n",
    "extracted_none = df[(df['label_extracted'] == 'none') & (df['label_human'].isna())].copy()\n",
    "\n",
    "min_gendered = min(len(female_all), len(male_all))\n",
    "target_none = min_gendered\n",
    "\n",
    "print(f\"\\nNone class sources:\")\n",
    "print(f\"  Human-coded none: {len(human_none)}\")\n",
    "print(f\"  Extracted none (unlabeled): {len(extracted_none)}\")\n",
    "\n",
    "total_none_available = len(human_none) + len(extracted_none)\n",
    "if total_none_available < MIN_CLASS_SIZE:\n",
    "    raise ValueError(f\"Insufficient 'none' samples. Need at least {MIN_CLASS_SIZE}.\")\n",
    "\n",
    "if len(human_none) >= target_none:\n",
    "    none_all = human_none.sample(n=target_none, random_state=RANDOM_STATE)\n",
    "else:\n",
    "    remaining = target_none - len(human_none)\n",
    "    sampled_none = extracted_none.sample(\n",
    "        n=min(remaining, len(extracted_none)), random_state=RANDOM_STATE\n",
    "    )\n",
    "    none_all = pd.concat([human_none, sampled_none])\n",
    "\n",
    "print(f\"  Final none class: {len(none_all)}\")\n",
    "\n",
    "# Balance\n",
    "min_class = min(len(female_all), len(male_all), len(none_all))\n",
    "print(f\"\\nBalancing to: {min_class} per class\")\n",
    "\n",
    "if min_class < MIN_CLASS_SIZE:\n",
    "    raise ValueError(f\"After balancing, class size ({min_class}) < minimum ({MIN_CLASS_SIZE})\")\n",
    "\n",
    "female_balanced = female_all.sample(n=min_class, random_state=RANDOM_STATE)\n",
    "male_balanced = male_all.sample(n=min_class, random_state=RANDOM_STATE)\n",
    "none_balanced = none_all.sample(n=min_class, random_state=RANDOM_STATE)\n",
    "\n",
    "ml_data = pd.concat([female_balanced, male_balanced, none_balanced]).copy()\n",
    "\n",
    "ml_data['target'] = ml_data['label_extracted'].map({'female': 0, 'male': 1})\n",
    "ml_data.loc[ml_data['target'].isna(), 'target'] = 2\n",
    "ml_data['target'] = ml_data['target'].astype(int)\n",
    "\n",
    "print(f\"\\n✓ Training data: {len(ml_data)} products\")\n",
    "print(f\"  Class distribution: {ml_data['target'].value_counts().sort_index().to_dict()}\")\n",
    "print(f\"  (0=female, 1=male, 2=none)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: TRAIN-TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    ml_data.index, test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE, stratify=ml_data['target']\n",
    ")\n",
    "\n",
    "train_data = ml_data.loc[train_idx].copy()\n",
    "test_data = ml_data.loc[test_idx].copy()\n",
    "\n",
    "print(f\"Train: {len(train_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")\n",
    "print(f\"Train distribution: {train_data['target'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def clean_text_remove_gender(text, remove_words=ALL_GENDER_KEYWORDS):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    for word in remove_words:\n",
    "        text = re.sub(r'\\b' + word + r'\\b', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "all_datasets = [train_data, test_data, df]\n",
    "for dataset in all_datasets:\n",
    "    dataset['breadcrumb_clean'] = dataset[COL_BREADCRUMB].apply(clean_text_remove_gender)\n",
    "    dataset['description_clean'] = dataset[COL_DESC].apply(clean_text_remove_gender)\n",
    "\n",
    "# --- Price features ---\n",
    "price_features = ['feat_price_log', 'feat_unit_price']\n",
    "for dataset in all_datasets:\n",
    "    dataset['feat_price'] = pd.to_numeric(dataset[COL_PRICE], errors='coerce')\n",
    "    dataset['feat_price_log'] = np.log1p(dataset['feat_price'])\n",
    "    if COL_UNIT_PRICE in dataset.columns:\n",
    "        dataset['feat_unit_price'] = (\n",
    "            dataset[COL_UNIT_PRICE].astype(str).str.extract(r'([\\d.]+)')[0].astype(float)\n",
    "        )\n",
    "    else:\n",
    "        dataset['feat_unit_price'] = 0\n",
    "print(f\"✓ Price features: {len(price_features)}\")\n",
    "\n",
    "# --- Store encoding ---\n",
    "store_encoder = LabelEncoder()\n",
    "all_stores = pd.concat([d[COL_STORE] for d in all_datasets]).fillna('unknown')\n",
    "store_encoder.fit(all_stores.unique())\n",
    "\n",
    "def encode_stores(data, encoder):\n",
    "    stores = data[COL_STORE].fillna('unknown')\n",
    "    encoded = []\n",
    "    for s in stores:\n",
    "        if s in encoder.classes_:\n",
    "            encoded.append(encoder.transform([s])[0])\n",
    "        else:\n",
    "            encoded.append(-1)\n",
    "    return np.array(encoded)\n",
    "\n",
    "for dataset in all_datasets:\n",
    "    dataset['store_encoded'] = encode_stores(dataset, store_encoder)\n",
    "\n",
    "n_stores = len(store_encoder.classes_) + 1\n",
    "print(f\"✓ Store features: {n_stores}\")\n",
    "\n",
    "# --- TF-IDF (fit on train only) ---\n",
    "breadcrumb_vectorizer = TfidfVectorizer(\n",
    "    max_features=200, min_df=5, max_df=0.9,\n",
    "    ngram_range=(1, 2), stop_words='english'\n",
    ")\n",
    "breadcrumb_vectorizer.fit(train_data['breadcrumb_clean'])\n",
    "print(f\"✓ Breadcrumb TF-IDF: {len(breadcrumb_vectorizer.get_feature_names_out())} features\")\n",
    "\n",
    "description_vectorizer = TfidfVectorizer(\n",
    "    max_features=500, min_df=5, max_df=0.9,\n",
    "    ngram_range=(1, 2), stop_words='english'\n",
    ")\n",
    "description_vectorizer.fit(train_data['description_clean'])\n",
    "print(f\"✓ Description TF-IDF: {len(description_vectorizer.get_feature_names_out())} features\")\n",
    "\n",
    "# --- Color features (merged on product_id, not index) ---\n",
    "color_feature_cols = []\n",
    "for color_name in STANDARD_COLORS.keys():\n",
    "    for i in range(1, N_COLORS + 1):\n",
    "        color_feature_cols.append(f'feat_color{i}_{color_name}')\n",
    "\n",
    "# Initialize to 0\n",
    "for dataset in all_datasets:\n",
    "    for col in color_feature_cols:\n",
    "        dataset[col] = 0.0\n",
    "\n",
    "if len(color_df) > 0 and COL_PRODUCT_ID in color_df.columns:\n",
    "    # Build a lookup: product_id → color features dict\n",
    "    color_lookup = {}\n",
    "    for _, row in color_df.iterrows():\n",
    "        pid = row[COL_PRODUCT_ID]\n",
    "        feats = {}\n",
    "        for i in range(1, N_COLORS + 1):\n",
    "            cname = row.get(f'color{i}_name')\n",
    "            cweight = row.get(f'color{i}_weight')\n",
    "            if pd.notna(cname) and cname in STANDARD_COLORS and pd.notna(cweight):\n",
    "                feats[f'feat_color{i}_{cname}'] = cweight\n",
    "        if feats:\n",
    "            color_lookup[pid] = feats\n",
    "\n",
    "    print(f\"  Color lookup built: {len(color_lookup):,} products\")\n",
    "\n",
    "    # Apply to each dataset via product_id\n",
    "    for dataset in all_datasets:\n",
    "        matched = 0\n",
    "        for idx, row in dataset.iterrows():\n",
    "            pid = row[COL_PRODUCT_ID]\n",
    "            if pid in color_lookup:\n",
    "                for col, val in color_lookup[pid].items():\n",
    "                    dataset.at[idx, col] = val\n",
    "                matched += 1\n",
    "        # Only print for train/test, not full df (too noisy)\n",
    "        if len(dataset) < 10000:\n",
    "            print(f\"  Color features filled for {matched}/{len(dataset)} rows\")\n",
    "\n",
    "    train_has_color = sum(1 for _, r in train_data.iterrows() if r[COL_PRODUCT_ID] in color_lookup)\n",
    "else:\n",
    "    color_lookup = {}\n",
    "    train_has_color = 0\n",
    "\n",
    "print(f\"✓ Color features: {len(color_feature_cols)} \"\n",
    "      f\"(available for {train_has_color}/{len(train_data)} train samples)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: BUILD FEATURE MATRICES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: BUILD FEATURE MATRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def build_feature_matrix(data, bc_vec, desc_vec, color_cols, price_cols,\n",
    "                         n_stores, include_colors=True):\n",
    "    feature_names = []\n",
    "    blocks = []\n",
    "\n",
    "    # Price\n",
    "    X_price = data[price_cols].fillna(0).values\n",
    "    blocks.append(csr_matrix(X_price))\n",
    "    feature_names.extend(price_cols)\n",
    "\n",
    "    # Store (one-hot)\n",
    "    store_enc = data['store_encoded'].values\n",
    "    X_store = np.zeros((len(data), n_stores))\n",
    "    for i, s in enumerate(store_enc):\n",
    "        if s >= 0:\n",
    "            X_store[i, s] = 1\n",
    "        else:\n",
    "            X_store[i, -1] = 1\n",
    "    blocks.append(csr_matrix(X_store))\n",
    "    feature_names.extend([f'store_{i}' for i in range(n_stores)])\n",
    "\n",
    "    # Breadcrumb TF-IDF\n",
    "    X_bc = bc_vec.transform(data['breadcrumb_clean'])\n",
    "    blocks.append(X_bc)\n",
    "    feature_names.extend([f'bc_{f}' for f in bc_vec.get_feature_names_out()])\n",
    "\n",
    "    # Description TF-IDF\n",
    "    X_desc = desc_vec.transform(data['description_clean'])\n",
    "    blocks.append(X_desc)\n",
    "    feature_names.extend([f'desc_{f}' for f in desc_vec.get_feature_names_out()])\n",
    "\n",
    "    # Color (optional)\n",
    "    if include_colors:\n",
    "        X_color = data[color_cols].values\n",
    "        blocks.append(csr_matrix(X_color))\n",
    "        feature_names.extend(color_cols)\n",
    "\n",
    "    X = hstack(blocks)\n",
    "    return X, feature_names\n",
    "\n",
    "X_train, feature_names = build_feature_matrix(\n",
    "    train_data, breadcrumb_vectorizer, description_vectorizer,\n",
    "    color_feature_cols, price_features, n_stores, include_colors=True\n",
    ")\n",
    "X_test, _ = build_feature_matrix(\n",
    "    test_data, breadcrumb_vectorizer, description_vectorizer,\n",
    "    color_feature_cols, price_features, n_stores, include_colors=True\n",
    ")\n",
    "\n",
    "y_train = train_data['target'].values\n",
    "y_test = test_data['target'].values\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "assert X_train.shape[1] == len(feature_names)\n",
    "print(f\"✓ Feature alignment validated\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: TRAIN MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 8: TRAIN MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "# --- L1 Logistic Regression ---\n",
    "print(\"\\n--- Logistic Regression (L1) ---\")\n",
    "model_l1 = LogisticRegressionCV(\n",
    "    cv=CV_FOLDS, penalty='l1', solver='saga', max_iter=2000,\n",
    "    multi_class='multinomial', class_weight='balanced', random_state=RANDOM_STATE\n",
    ")\n",
    "model_l1.fit(X_train, y_train)\n",
    "y_pred_l1 = model_l1.predict(X_test)\n",
    "acc_l1 = accuracy_score(y_test, y_pred_l1)\n",
    "f1_l1 = f1_score(y_test, y_pred_l1, average='weighted')\n",
    "print(f\"Accuracy: {acc_l1:.4f}, F1: {f1_l1:.4f}\")\n",
    "results.append({'Model': 'L1 (LASSO)', 'Accuracy': acc_l1, 'F1_weighted': f1_l1})\n",
    "\n",
    "# --- L2 Logistic Regression ---\n",
    "print(\"\\n--- Logistic Regression (L2) ---\")\n",
    "model_l2 = LogisticRegressionCV(\n",
    "    cv=CV_FOLDS, penalty='l2', solver='lbfgs', max_iter=2000,\n",
    "    multi_class='multinomial', class_weight='balanced', random_state=RANDOM_STATE\n",
    ")\n",
    "model_l2.fit(X_train, y_train)\n",
    "y_pred_l2 = model_l2.predict(X_test)\n",
    "acc_l2 = accuracy_score(y_test, y_pred_l2)\n",
    "f1_l2 = f1_score(y_test, y_pred_l2, average='weighted')\n",
    "print(f\"Accuracy: {acc_l2:.4f}, F1: {f1_l2:.4f}\")\n",
    "results.append({'Model': 'L2 (Ridge)', 'Accuracy': acc_l2, 'F1_weighted': f1_l2})\n",
    "\n",
    "# --- Random Forest ---\n",
    "print(\"\\n--- Random Forest ---\")\n",
    "model_rf = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=10, min_samples_leaf=5,\n",
    "    class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "print(f\"Accuracy: {acc_rf:.4f}, F1: {f1_rf:.4f}\")\n",
    "results.append({'Model': 'Random Forest', 'Accuracy': acc_rf, 'F1_weighted': f1_rf})\n",
    "\n",
    "# --- Histogram Gradient Boosting ---\n",
    "print(\"\\n--- Histogram Gradient Boosting ---\")\n",
    "MAX_HGB_SAMPLES = 50000\n",
    "if X_train.shape[0] > MAX_HGB_SAMPLES:\n",
    "    sample_idx = np.random.choice(X_train.shape[0], MAX_HGB_SAMPLES, replace=False)\n",
    "    X_train_hgb = X_train[sample_idx].toarray()\n",
    "    y_train_hgb = y_train[sample_idx]\n",
    "else:\n",
    "    X_train_hgb = X_train.toarray()\n",
    "    y_train_hgb = y_train\n",
    "\n",
    "model_hgb = HistGradientBoostingClassifier(\n",
    "    max_iter=200, max_depth=10, learning_rate=0.1, random_state=RANDOM_STATE\n",
    ")\n",
    "model_hgb.fit(X_train_hgb, y_train_hgb)\n",
    "y_pred_hgb = model_hgb.predict(X_test.toarray())\n",
    "acc_hgb = accuracy_score(y_test, y_pred_hgb)\n",
    "f1_hgb = f1_score(y_test, y_pred_hgb, average='weighted')\n",
    "print(f\"Accuracy: {acc_hgb:.4f}, F1: {f1_hgb:.4f}\")\n",
    "results.append({'Model': 'Hist Gradient Boosting', 'Accuracy': acc_hgb, 'F1_weighted': f1_hgb})\n",
    "\n",
    "# --- SVM ---\n",
    "print(\"\\n--- SVM (RBF) ---\")\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_svm = SVC(\n",
    "    kernel='rbf', C=1.0, gamma='scale', class_weight='balanced',\n",
    "    probability=True, random_state=RANDOM_STATE\n",
    ")\n",
    "model_svm.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = model_svm.predict(X_test_scaled)\n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "print(f\"Accuracy: {acc_svm:.4f}, F1: {f1_svm:.4f}\")\n",
    "results.append({'Model': 'SVM', 'Accuracy': acc_svm, 'F1_weighted': f1_svm})\n",
    "\n",
    "# Results table\n",
    "results_df = pd.DataFrame(results).sort_values('F1_weighted', ascending=False)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "results_df.to_csv(OUTPUT_DIR / 'model_comparison.csv', index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: BEST MODEL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST MODEL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_name = results_df.iloc[0]['Model']\n",
    "print(f\"Best model: {best_name}\")\n",
    "\n",
    "model_map = {\n",
    "    'L1': (model_l1, y_pred_l1),\n",
    "    'L2': (model_l2, y_pred_l2),\n",
    "    'Random': (model_rf, y_pred_rf),\n",
    "    'Hist': (model_hgb, y_pred_hgb),\n",
    "    'SVM': (model_svm, y_pred_svm),\n",
    "}\n",
    "for key, (model, preds) in model_map.items():\n",
    "    if key in best_name:\n",
    "        best_model = model\n",
    "        y_pred_best = preds\n",
    "        break\n",
    "\n",
    "if len(np.unique(y_test)) == 3 and len(np.unique(y_pred_best)) > 0:\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_best, target_names=['female', 'male', 'none']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_best, labels=[0, 1, 2])\n",
    "print(f\"            Predicted\")\n",
    "print(f\"            female  male  none\")\n",
    "for i, label in enumerate(['female', 'male', 'none']):\n",
    "    row = cm[i] if i < len(cm) else [0, 0, 0]\n",
    "    print(f\"Actual {label:6s}  {row[0]:4d}  {row[1]:4d}  {row[2]:4d}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE (L1 Model)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coef_female': model_l1.coef_[0],\n",
    "    'coef_male': model_l1.coef_[1],\n",
    "    'coef_none': model_l1.coef_[2]\n",
    "})\n",
    "importance_df['max_abs'] = importance_df[['coef_female', 'coef_male', 'coef_none']].abs().max(axis=1)\n",
    "importance_df = importance_df.sort_values('max_abs', ascending=False)\n",
    "\n",
    "for label, col in [('FEMALE', 'coef_female'), ('MALE', 'coef_male'), ('NONE', 'coef_none')]:\n",
    "    top = importance_df[importance_df[col] > 0].nlargest(15, col)\n",
    "    print(f\"\\nTop 15 {label} features:\")\n",
    "    for _, row in top.iterrows():\n",
    "        print(f\"  {row['feature']:40s}: {row[col]:+.4f}\")\n",
    "\n",
    "importance_df.to_csv(OUTPUT_DIR / 'feature_importance.csv', index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 11: PREDICT ON ALL PRODUCTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 11: PREDICT ON ALL PRODUCTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_all, _ = build_feature_matrix(\n",
    "    df, breadcrumb_vectorizer, description_vectorizer,\n",
    "    color_feature_cols, price_features, n_stores, include_colors=True\n",
    ")\n",
    "print(f\"Full dataset: {X_all.shape}\")\n",
    "\n",
    "df['ml_prob_female'] = model_l1.predict_proba(X_all)[:, 0]\n",
    "df['ml_prob_male'] = model_l1.predict_proba(X_all)[:, 1]\n",
    "df['ml_prob_none'] = model_l1.predict_proba(X_all)[:, 2]\n",
    "df['ml_pred'] = model_l1.predict(X_all)\n",
    "df['ml_pred_label'] = df['ml_pred'].map({0: 'female', 1: 'male', 2: 'none'})\n",
    "df['ml_confidence'] = df[['ml_prob_female', 'ml_prob_male', 'ml_prob_none']].max(axis=1)\n",
    "\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(df['ml_pred_label'].value_counts())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 12: COLOR FEATURE IMPACT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 12: COLOR FEATURE IMPACT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_train_no_color, features_no_color = build_feature_matrix(\n",
    "    train_data, breadcrumb_vectorizer, description_vectorizer,\n",
    "    color_feature_cols, price_features, n_stores, include_colors=False\n",
    ")\n",
    "X_test_no_color, _ = build_feature_matrix(\n",
    "    test_data, breadcrumb_vectorizer, description_vectorizer,\n",
    "    color_feature_cols, price_features, n_stores, include_colors=False\n",
    ")\n",
    "\n",
    "model_no_color = LogisticRegressionCV(\n",
    "    cv=CV_FOLDS, penalty='l1', solver='saga', max_iter=2000,\n",
    "    multi_class='multinomial', class_weight='balanced', random_state=RANDOM_STATE\n",
    ")\n",
    "model_no_color.fit(X_train_no_color, y_train)\n",
    "y_pred_no_color = model_no_color.predict(X_test_no_color)\n",
    "\n",
    "acc_no_color = accuracy_score(y_test, y_pred_no_color)\n",
    "f1_no_color = f1_score(y_test, y_pred_no_color, average='weighted')\n",
    "\n",
    "print(f\"WITH colors:    Accuracy={acc_l1:.4f}, F1={f1_l1:.4f}\")\n",
    "print(f\"WITHOUT colors: Accuracy={acc_no_color:.4f}, F1={f1_no_color:.4f}\")\n",
    "print(f\"Color impact:   Accuracy {'+' if acc_l1 > acc_no_color else ''}\"\n",
    "      f\"{(acc_l1-acc_no_color)*100:.2f}pp, \"\n",
    "      f\"F1 {'+' if f1_l1 > f1_no_color else ''}{(f1_l1-f1_no_color)*100:.2f}pp\")\n",
    "\n",
    "# Subset analysis: only products that actually have color data\n",
    "if len(color_lookup) > 0:\n",
    "    test_with_color = test_data[test_data[COL_PRODUCT_ID].isin(color_lookup)]\n",
    "\n",
    "    if len(test_with_color) >= MIN_TEST_SAMPLES:\n",
    "        print(f\"\\nOn color-available subset ({len(test_with_color)} test samples):\")\n",
    "\n",
    "        X_sub_with, _ = build_feature_matrix(\n",
    "            test_with_color, breadcrumb_vectorizer, description_vectorizer,\n",
    "            color_feature_cols, price_features, n_stores, include_colors=True\n",
    "        )\n",
    "        X_sub_without, _ = build_feature_matrix(\n",
    "            test_with_color, breadcrumb_vectorizer, description_vectorizer,\n",
    "            color_feature_cols, price_features, n_stores, include_colors=False\n",
    "        )\n",
    "        y_sub = test_with_color['target'].values\n",
    "\n",
    "        acc_sub_with = accuracy_score(y_sub, model_l1.predict(X_sub_with))\n",
    "        acc_sub_without = accuracy_score(y_sub, model_no_color.predict(X_sub_without))\n",
    "        print(f\"  WITH colors:    Accuracy={acc_sub_with:.4f}\")\n",
    "        print(f\"  WITHOUT colors: Accuracy={acc_sub_without:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Too few test samples with color data ({len(test_with_color)})\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 13: VALIDATION VS HUMAN LABELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 13: VALIDATION VS HUMAN LABELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "human_labeled = df[df['label_human'].notna()].copy()\n",
    "print(f\"Products with human labels: {len(human_labeled)}\")\n",
    "\n",
    "if len(human_labeled) > 0:\n",
    "    human_labeled['human_encoded'] = human_labeled['label_human'].map({\n",
    "        'female': 0, 'male': 1, 'none': 2\n",
    "    })\n",
    "    valid = human_labeled[human_labeled['human_encoded'].notna()]\n",
    "\n",
    "    if len(valid) >= MIN_TEST_SAMPLES:\n",
    "        acc_human = accuracy_score(valid['human_encoded'], valid['ml_pred'])\n",
    "        print(f\"Accuracy vs human: {acc_human:.4f}\")\n",
    "\n",
    "        if len(valid['human_encoded'].unique()) >= 2:\n",
    "            print(\"\\nConfusion (ML vs Human):\")\n",
    "            cm_h = confusion_matrix(valid['human_encoded'], valid['ml_pred'], labels=[0, 1, 2])\n",
    "            print(f\"            ML Predicted\")\n",
    "            print(f\"            female  male  none\")\n",
    "            for i, label in enumerate(['female', 'male', 'none']):\n",
    "                print(f\"Human {label:6s}  {cm_h[i,0]:4d}  {cm_h[i,1]:4d}  {cm_h[i,2]:4d}\")\n",
    "    else:\n",
    "        print(f\"⚠ Too few validated samples ({len(valid)})\")\n",
    "else:\n",
    "    print(\"⚠ No human-labeled data available\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 14: IMPLICIT GENDERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 14: IMPLICIT GENDERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "implicit_female = df[\n",
    "    (df['label_extracted'] == 'none') &\n",
    "    (df['ml_pred_label'] == 'female') &\n",
    "    (df['ml_confidence'] > 0.5)\n",
    "]\n",
    "implicit_male = df[\n",
    "    (df['label_extracted'] == 'none') &\n",
    "    (df['ml_pred_label'] == 'male') &\n",
    "    (df['ml_confidence'] > 0.5)\n",
    "]\n",
    "predicted_none = df[df['ml_pred_label'] == 'none']\n",
    "\n",
    "print(f\"Implicit female (>50% conf): {len(implicit_female):,}\")\n",
    "print(f\"Implicit male (>50% conf): {len(implicit_male):,}\")\n",
    "print(f\"Predicted none: {len(predicted_none):,}\")\n",
    "\n",
    "if len(implicit_female) > 0:\n",
    "    print(f\"\\nSample implicit FEMALE:\")\n",
    "    for _, row in implicit_female.head(5).iterrows():\n",
    "        print(f\"  {row[COL_NAME][:60]}... (conf: {row['ml_confidence']:.2f})\")\n",
    "\n",
    "if len(implicit_male) > 0:\n",
    "    print(f\"\\nSample implicit MALE:\")\n",
    "    for _, row in implicit_male.head(5).iterrows():\n",
    "        print(f\"  {row[COL_NAME][:60]}... (conf: {row['ml_confidence']:.2f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 15: EXPORT VALIDATION SAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 15: EXPORT VALIDATION SAMPLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "already_labeled = set()\n",
    "if COL_PRODUCT_ID in human_coded.columns:\n",
    "    already_labeled.update(human_coded[COL_PRODUCT_ID].values)\n",
    "if len(your_labeled) > 0 and COL_PRODUCT_ID in your_labeled.columns:\n",
    "    already_labeled.update(your_labeled[COL_PRODUCT_ID].values)\n",
    "\n",
    "available = df[\n",
    "    (~df[COL_PRODUCT_ID].isin(already_labeled)) &\n",
    "    (df[COL_IMAGE].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"Available for validation: {len(available):,}\")\n",
    "\n",
    "if len(available) == 0:\n",
    "    print(\"⚠ No products available for validation\")\n",
    "    validation_export = pd.DataFrame()\n",
    "else:\n",
    "    N_PER = 85\n",
    "    samples = []\n",
    "    for pred, label in [(0, 'female'), (1, 'male'), (2, 'none')]:\n",
    "        pool = available[available['ml_pred'] == pred]\n",
    "        n = min(N_PER, len(pool))\n",
    "        if n > 0:\n",
    "            samples.append(pool.sample(n=n, random_state=RANDOM_STATE))\n",
    "            print(f\"  Sampled {n} {label}\")\n",
    "\n",
    "    if samples:\n",
    "        validation = pd.concat(samples).sample(frac=1, random_state=RANDOM_STATE)\n",
    "\n",
    "        export_cols = [\n",
    "            COL_PRODUCT_ID, COL_NAME, COL_DESC, COL_BREADCRUMB, COL_IMAGE,\n",
    "            COL_URL, COL_PRICE, 'label_extracted', 'ml_pred_label',\n",
    "            'ml_prob_female', 'ml_prob_male', 'ml_prob_none', 'ml_confidence'\n",
    "        ]\n",
    "        export_cols = [c for c in export_cols if c in validation.columns]\n",
    "\n",
    "        validation_export = validation[export_cols].copy()\n",
    "        validation_export['manual_gender'] = ''\n",
    "        validation_export['manual_confidence'] = ''\n",
    "        validation_export['manual_notes'] = ''\n",
    "\n",
    "        output_file = OUTPUT_DIR / 'validation_sample.csv'\n",
    "        validation_export.to_csv(output_file, index=True)\n",
    "        print(f\"\\n✓ Saved: {output_file} ({len(validation_export)} products)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE SUMMARY (v4)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATA:\n",
    "  Original: {original_count:,}\n",
    "  After filtering: {len(df):,}\n",
    "  Excluded categories: {excluded_count:,}\n",
    "\n",
    "TRAINING:\n",
    "  Total samples: {len(ml_data):,} (balanced 3-class)\n",
    "  Train: {len(train_data):,}\n",
    "  Test: {len(test_data):,}\n",
    "  Products with colors: {len(color_df):,} (Morrisons only — Tesco/ASDA CDN links expired)\n",
    "\n",
    "BEST MODEL: {best_name} (F1: {results_df.iloc[0]['F1_weighted']:.4f})\n",
    "\n",
    "COLOR IMPACT:\n",
    "  With colors:    F1={f1_l1:.4f}\n",
    "  Without colors: F1={f1_no_color:.4f}\n",
    "\n",
    "PREDICTIONS:\n",
    "  Female: {(df['ml_pred_label'] == 'female').sum():,}\n",
    "  Male: {(df['ml_pred_label'] == 'male').sum():,}\n",
    "  None: {(df['ml_pred_label'] == 'none').sum():,}\n",
    "\n",
    "IMPLICIT GENDERING:\n",
    "  Implicit female: {len(implicit_female):,}\n",
    "  Implicit male: {len(implicit_male):,}\n",
    "\n",
    "OUTPUT:\n",
    "  {OUTPUT_DIR}/\n",
    "  ├── model_comparison.csv\n",
    "  ├── feature_importance.csv\n",
    "  └── validation_sample.csv\n",
    "\n",
    "NOTE: Color features are limited to Morrisons products (~5,600 of 12,800).\n",
    "      Tesco and ASDA image CDN links are expired. Color-based findings\n",
    "      should be interpreted with this coverage limitation in mind.\n",
    "\"\"\")\n",
    "\n",
    "summary = {\n",
    "    'version': '4.0',\n",
    "    'data': {\n",
    "        'original': original_count,\n",
    "        'filtered': int(len(df)),\n",
    "        'excluded': int(excluded_count),\n",
    "        'training_samples': int(len(ml_data)),\n",
    "        'color_samples': int(len(color_df)),\n",
    "        'color_coverage_note': 'Morrisons only; Tesco/ASDA CDN links expired'\n",
    "    },\n",
    "    'models': results,\n",
    "    'color_impact': {\n",
    "        'with_colors_f1': float(f1_l1),\n",
    "        'without_colors_f1': float(f1_no_color)\n",
    "    },\n",
    "    'predictions': {\n",
    "        'female': int((df['ml_pred_label'] == 'female').sum()),\n",
    "        'male': int((df['ml_pred_label'] == 'male').sum()),\n",
    "        'none': int((df['ml_pred _label'] == 'none').sum())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Pipeline complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28a76ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PINK TAX REGRESSION ANALYSIS (v2)\n",
      "======================================================================\n",
      "Loaded 21,436 products\n",
      "After category filter: 12,832\n",
      "With valid prices: 12,832\n",
      "\n",
      "Gender distribution:\n",
      "  female: 1,075  (mean £7.03, median £4.00)\n",
      "  male: 844  (mean £8.15, median £4.50)\n",
      "  none: 10,913  (mean £10.66, median £5.00)\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS SAMPLE: FEMALE vs MALE\n",
      "======================================================================\n",
      "N = 1,919  (F: 1075, M: 844)\n",
      "Mean price  — F: £7.03, M: £8.15\n",
      "Median price — F: £4.00, M: £4.50\n",
      "  Categories (broad) with ≥3F + ≥3M: 11\n",
      "  Categories (mid) with ≥3F + ≥3M: 11\n",
      "  Categories (fine) with ≥3F + ≥3M: 11\n",
      "\n",
      "Spec 1: Raw gap\n",
      "  (1) Raw gap: coef=-0.0975 (-9.3%), SE=0.0411, p=0.0178**, R²=0.003, N=1919\n",
      "\n",
      "Spec 2: + Store FE\n",
      "  (2) + Store FE: coef=-0.1036 (-9.8%), SE=0.0412, p=0.0118**, R²=0.006, N=1919\n",
      "\n",
      "Spec 3: + Broad category FE (N cats: 96)\n",
      "  (3) + Broad cat FE: coef=+0.0803 (+8.4%), SE=0.0851, p=0.3454, R²=0.672, N=1374\n",
      "\n",
      "Spec 4: + Mid category FE (N cats: 96)\n",
      "  (4) + Mid cat FE: coef=+0.0803 (+8.4%), SE=0.0851, p=0.3454, R²=0.672, N=1374\n",
      "\n",
      "Spec 5: + Fine category FE (N cats: 96)\n",
      "  (5) + Fine cat FE: coef=+0.0803 (+8.4%), SE=0.0851, p=0.3454, R²=0.672, N=1374\n",
      "\n",
      "Spec 6: + Description TF-IDF\n",
      "  (6) + Description: coef=+0.0367 (+3.7%), SE=0.0675, p=0.5868, R²=0.738, N=1374\n",
      "\n",
      "Spec 7: Female × Store interaction\n",
      "  Main effect (is_female): -0.0412 (p=0.5918)\n",
      "  Store 3: total female effect = -0.7046 (-50.6%), interaction p=0.0000\n",
      "  Store 4: total female effect = +0.2767 (+31.9%), interaction p=0.0969\n",
      "\n",
      "Spec 8: Unit price regression\n",
      "  (8) Unit price: coef=+0.0067 (+0.7%), SE=0.0677, p=0.9218, R²=0.854, N=743\n",
      "\n",
      "Spec 9: Three-way comparison (none = reference)\n",
      "  is_female: -0.1794 (p=0.0000)\n",
      "  is_male:   -0.0299 (p=0.3745)\n",
      "  F vs M:    -13.9%\n",
      "\n",
      "======================================================================\n",
      "QUANTILE REGRESSION\n",
      "======================================================================\n",
      "  Q0.10: coef=-0.1285 (-12.1%), p=0.0440**\n",
      "  Q0.25: coef=-0.0000 (-0.0%), p=0.9999\n",
      "  Q0.50: coef=-0.1054 (-10.0%), p=0.0217**\n",
      "  Q0.75: coef=+0.0000 (+0.0%), p=1.0000\n",
      "  Q0.90: coef=-0.2226 (-20.0%), p=0.0023***\n",
      "\n",
      "  With mid-category controls:\n",
      "  Q0.10: coef=+0.3578 (+43.0%), p=0.0000***\n",
      "  Q0.25: coef=+0.2231 (+25.0%), p=0.0002***\n",
      "  Q0.50: coef=+0.0000 (+0.0%), p=1.0000\n",
      "  Q0.75: coef=+0.0000 (+0.0%), p=1.0000\n",
      "  Q0.90: coef=+0.0000 (+0.0%), p=1.0000\n",
      "\n",
      "======================================================================\n",
      "WITHIN-CATEGORY ANALYSIS (bootstrap CIs)\n",
      "======================================================================\n",
      "Categories with both genders (≥3 each): 11\n",
      "\n",
      "Within-category price gap (F vs M):\n",
      "  Weighted mean: +26.2%\n",
      "  Trimmed mean:  +3.0%\n",
      "  Median:        +7.4%\n",
      "\n",
      "  category                                        F   M      gap           95% CI  sig\n",
      "  --------------------------------------------------------------------------------\n",
      "  {'bc1': 'toiletries & beauty', 'bc2': 'incont  12   3  +259.4% [+123.6, +499.4]   *\n",
      "  {'bc1': 'toiletries & beauty', 'bc2': 'giftin   3   9   +30.2% [ -46.1, +165.6]    \n",
      "  {'bc1': 'marketplace', 'bc2': 'greeting cards  11   9   +25.1% [ +11.1,  +43.0]   *\n",
      "  {'bc1': 'toiletries & beauty', 'bc2': 'incont   5   3   +16.3% [ -41.0, +369.2]    \n",
      "  {'bc1': 'marketplace', 'bc2': 'greeting cards  19   7    +9.5% [  -1.5,  +23.6]    \n",
      "  {'bc1': 'marketplace', 'bc2': 'greeting cards  12   4    +7.4% [  -2.0,  +28.3]    \n",
      "  {'bc1': 'marketplace', 'bc2': 'greeting cards   5   5    +2.0% [ -14.2,  +25.4]    \n",
      "  {'bc1': 'baby & toddler', 'bc2': 'nappies & p   3   3    +0.0% [ -43.1,  +75.8]    \n",
      "  {'bc1': 'marketplace', 'bc2': 'greeting cards   3   3   -28.3% [ -67.4, +101.8]    \n",
      "  {'bc1': 'health, wellbeing & medicines', 'bc2  11   4   -35.4% [ -62.8,  +26.4]    \n",
      "  {'bc1': 'health & beauty', 'bc2': 'shower', '   3  10   -55.1% [ -67.2,  -34.1]   *\n",
      "\n",
      "======================================================================\n",
      "PINK TAX BY STORE\n",
      "======================================================================\n",
      "store               F     M   mean F   mean M      gap        p\n",
      "-----------------------------------------------------------------\n",
      "Store 4           449   404 £  7.01 £  7.49    +0.8%  0.8938\n",
      "Store 3            29    34 £  5.41 £  5.31    -5.1%  0.8036\n",
      "Store 1           597   406 £  7.12 £  9.05   -18.6%  0.0003***\n",
      "\n",
      "======================================================================\n",
      "REGRESSION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Spec                          Coef     %gap             95% CI        p     R²      N\n",
      "--------------------------------------------------------------------------------\n",
      "(1) Raw gap               -0.0975    -9.3% [ -16.3,   -1.7]  0.0178**  0.003   1919\n",
      "(2) + Store FE            -0.1036    -9.8% [ -16.8,   -2.3]  0.0118**  0.006   1919\n",
      "(3) + Broad cat FE        +0.0803    +8.4% [  -8.3,  +28.0]  0.3454    0.672   1374\n",
      "(4) + Mid cat FE          +0.0803    +8.4% [  -8.3,  +28.0]  0.3454    0.672   1374\n",
      "(5) + Fine cat FE         +0.0803    +8.4% [  -8.3,  +28.0]  0.3454    0.672   1374\n",
      "(6) + Description         +0.0367    +3.7% [  -9.1,  +18.4]  0.5868    0.738   1374\n",
      "(8) Unit price            +0.0067    +0.7% [ -11.8,  +15.0]  0.9218    0.854    743\n",
      "\n",
      "======================================================================\n",
      "GENERATING CHARTS\n",
      "======================================================================\n",
      "✓ 01_coefficient_plot.png\n",
      "✓ 02_quantile_regression.png\n",
      "✓ 03_within_category_gaps.png\n",
      "✓ 04_price_distributions.png\n",
      "✓ 05_by_store.png\n",
      "✓ 06_scatter_by_category.png\n",
      "✓ 07_three_way_comparison.png\n",
      "✓ 08_category_composition.png\n",
      "✓ 09_gap_distribution.png\n",
      "✓ 10_r2_progression.png\n",
      "✓ 12_summary_dashboard.png\n",
      "\n",
      "======================================================================\n",
      "OUTPUT FILES\n",
      "======================================================================\n",
      "  01_coefficient_plot.png\n",
      "  02_quantile_regression.png\n",
      "  03_within_category_gaps.png\n",
      "  04_price_distributions.png\n",
      "  05_by_store.png\n",
      "  06_scatter_by_category.png\n",
      "  07_three_way_comparison.png\n",
      "  08_category_composition.png\n",
      "  09_gap_distribution.png\n",
      "  10_r2_progression.png\n",
      "  12_summary_dashboard.png\n",
      "  pink_tax_by_store.csv\n",
      "  regression_summary.csv\n",
      "  within_category_gaps.csv\n",
      "\n",
      "✓ All outputs saved to /Users/leoss/Desktop/Portfolio/Website-/UK pink tax/Outputs/charts/pink_tax_regression_v2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PINK TAX REGRESSION ANALYSIS (v2)\n",
    "# ============================================================================\n",
    "#\n",
    "# Improvements over v1:\n",
    "#   - Fixed breadcrumb parsing (was printing dicts)\n",
    "#   - Added quantile regression (10th, 25th, 50th, 75th, 90th percentiles)\n",
    "#   - Added female × store interaction\n",
    "#   - Added bootstrap CIs for within-category gaps\n",
    "#   - Category matching at multiple granularity levels\n",
    "#   - Trimmed outlier categories from within-category averages\n",
    "#   - 10+ visualisations\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = Path('/Users/leoss')\n",
    "DATA_DIR = BASE_DIR / 'Downloads'\n",
    "OUTPUT_BASE = BASE_DIR / 'Desktop/Portfolio/Website-/UK pink tax/Outputs'\n",
    "\n",
    "PATH_MAIN_DATA = DATA_DIR / 'items_fin.csv'\n",
    "OUTPUT_DIR = OUTPUT_BASE / 'charts/pink_tax_regression_v2'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_BOOTSTRAP = 1000\n",
    "\n",
    "EXCLUDE_CATEGORIES = [\n",
    "    'food', 'grocery', 'groceries', 'snacks', 'drinks', 'beverages',\n",
    "    'pet food', 'pet supplies', 'cleaning', 'household', 'kitchen',\n",
    "    'office', 'stationery', 'electronics', 'tech', 'garden', 'automotive'\n",
    "]\n",
    "\n",
    "FEMALE_KEYWORDS = ['women', 'woman', 'female', 'ladies', 'lady', 'girls',\n",
    "                   'womens', \"women's\", 'femme', 'her', 'feminine', 'fem']\n",
    "MALE_KEYWORDS = ['men', 'man', 'male', 'gentleman', 'gentlemen', 'boys',\n",
    "                 'mens', \"men's\", 'homme', 'his', 'masculine']\n",
    "ALL_GENDER_KEYWORDS = set(FEMALE_KEYWORDS + MALE_KEYWORDS)\n",
    "\n",
    "COL_NAME = 'product_title_x'\n",
    "COL_DESC = 'description'\n",
    "COL_BREADCRUMB = 'standardized_breadcrumbs'\n",
    "COL_PRICE = 'price'\n",
    "COL_UNIT_PRICE = 'unit_price'\n",
    "COL_STORE = 'store_id'\n",
    "COL_PRODUCT_ID = 'product_id'\n",
    "\n",
    "# Chart style\n",
    "PALETTE = {'female': '#c44e52', 'male': '#4c72b0', 'none': '#8c8c8c'}\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': '#fafafa',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.linestyle': '--',\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 10,\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD AND PREPARE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PINK TAX REGRESSION ANALYSIS (v2)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df = pd.read_csv(PATH_MAIN_DATA, encoding='latin-1')\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "if 'unnamed:_0' in df.columns:\n",
    "    df = df.drop(columns=['unnamed:_0'])\n",
    "print(f\"Loaded {len(df):,} products\")\n",
    "\n",
    "# Filter categories\n",
    "def contains_excluded_category(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    return any(cat in str(text).lower() for cat in EXCLUDE_CATEGORIES)\n",
    "\n",
    "df = df[~df[COL_BREADCRUMB].apply(contains_excluded_category)].copy().reset_index(drop=True)\n",
    "print(f\"After category filter: {len(df):,}\")\n",
    "\n",
    "# Gender labels\n",
    "def extract_gender(text):\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return 'none'\n",
    "    text_lower = str(text).lower()\n",
    "    has_f = any(re.search(r'\\b' + kw + r'\\b', text_lower) for kw in FEMALE_KEYWORDS)\n",
    "    has_m = any(re.search(r'\\b' + kw + r'\\b', text_lower) for kw in MALE_KEYWORDS)\n",
    "    if has_f and not has_m:\n",
    "        return 'female'\n",
    "    elif has_m and not has_f:\n",
    "        return 'male'\n",
    "    elif has_f and has_m:\n",
    "        return 'both'\n",
    "    return 'none'\n",
    "\n",
    "for col_label, col_source in [('label_bc', COL_BREADCRUMB),\n",
    "                               ('label_name', COL_NAME),\n",
    "                               ('label_desc', COL_DESC)]:\n",
    "    df[col_label] = df[col_source].apply(extract_gender)\n",
    "\n",
    "def combine_labels(row):\n",
    "    for col in ['label_bc', 'label_name', 'label_desc']:\n",
    "        if row[col] in ['female', 'male']:\n",
    "            return row[col]\n",
    "    return 'none'\n",
    "\n",
    "df['gender'] = df.apply(combine_labels, axis=1)\n",
    "\n",
    "# Price\n",
    "df['price_num'] = pd.to_numeric(df[COL_PRICE], errors='coerce')\n",
    "df = df[df['price_num'].notna() & (df['price_num'] > 0)].copy()\n",
    "df['log_price'] = np.log(df['price_num'])\n",
    "\n",
    "# Unit price\n",
    "if COL_UNIT_PRICE in df.columns:\n",
    "    df['unit_price_num'] = df[COL_UNIT_PRICE].astype(str).str.extract(r'([\\d.]+)')[0].astype(float)\n",
    "\n",
    "# Store\n",
    "df['store'] = df[COL_STORE].fillna('unknown').astype(str)\n",
    "\n",
    "# Store name mapping (from store IDs)\n",
    "store_names = {}\n",
    "for sid in df['store'].unique():\n",
    "    sub = df[df['store'] == sid]\n",
    "    # Infer from breadcrumb or URL patterns\n",
    "    bc_sample = sub[COL_BREADCRUMB].dropna().head(10).str.lower()\n",
    "    if bc_sample.str.contains('morrisons').any():\n",
    "        store_names[sid] = 'Morrisons'\n",
    "    elif bc_sample.str.contains('tesco').any():\n",
    "        store_names[sid] = 'Tesco'\n",
    "    elif bc_sample.str.contains('asda').any():\n",
    "        store_names[sid] = 'ASDA'\n",
    "    else:\n",
    "        store_names[sid] = f'Store {sid}'\n",
    "\n",
    "df['store_name'] = df['store'].map(store_names)\n",
    "\n",
    "# ---- Breadcrumb parsing (FIXED) ----\n",
    "def parse_breadcrumb(text):\n",
    "    \"\"\"Extract clean category levels from breadcrumb string.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 'unknown', 'unknown', 'unknown'\n",
    "    # Handle both \" > \" and \" / \" separators\n",
    "    text = str(text).strip()\n",
    "    if ' > ' in text:\n",
    "        parts = [p.strip().lower() for p in text.split(' > ') if p.strip()]\n",
    "    elif ' / ' in text:\n",
    "        parts = [p.strip().lower() for p in text.split(' / ') if p.strip()]\n",
    "    else:\n",
    "        parts = [text.strip().lower()]\n",
    "\n",
    "    # Some breadcrumbs start with store name — skip if it matches known stores\n",
    "    known_stores = {'morrisons', 'tesco', 'asda', 'groceries', 'marketplace'}\n",
    "    while parts and parts[0] in known_stores:\n",
    "        parts = parts[1:]\n",
    "\n",
    "    level1 = parts[0] if len(parts) > 0 else 'unknown'\n",
    "    level2 = parts[1] if len(parts) > 1 else 'unknown'\n",
    "    level3 = parts[2] if len(parts) > 2 else 'unknown'\n",
    "    return level1, level2, level3\n",
    "\n",
    "df[['cat1', 'cat2', 'cat3']] = df[COL_BREADCRUMB].apply(\n",
    "    lambda x: pd.Series(parse_breadcrumb(x))\n",
    ")\n",
    "\n",
    "# Category labels at different granularities\n",
    "df['cat_broad'] = df['cat1']\n",
    "df['cat_mid'] = df['cat1'] + ' > ' + df['cat2']\n",
    "df['cat_fine'] = df['cat1'] + ' > ' + df['cat2'] + ' > ' + df['cat3']\n",
    "\n",
    "print(f\"With valid prices: {len(df):,}\")\n",
    "print(f\"\\nGender distribution:\")\n",
    "for g in ['female', 'male', 'none', 'both']:\n",
    "    sub = df[df['gender'] == g]\n",
    "    if len(sub) > 0:\n",
    "        print(f\"  {g}: {len(sub):,}  (mean £{sub['price_num'].mean():.2f}, \"\n",
    "              f\"median £{sub['price_num'].median():.2f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS SAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS SAMPLE: FEMALE vs MALE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "gendered = df[df['gender'].isin(['female', 'male'])].copy()\n",
    "gendered['is_female'] = (gendered['gender'] == 'female').astype(int)\n",
    "\n",
    "print(f\"N = {len(gendered):,}  (F: {gendered['is_female'].sum()}, \"\n",
    "      f\"M: {(1 - gendered['is_female']).sum()})\")\n",
    "print(f\"Mean price  — F: £{gendered.loc[gendered['is_female']==1, 'price_num'].mean():.2f}, \"\n",
    "      f\"M: £{gendered.loc[gendered['is_female']==0, 'price_num'].mean():.2f}\")\n",
    "print(f\"Median price — F: £{gendered.loc[gendered['is_female']==1, 'price_num'].median():.2f}, \"\n",
    "      f\"M: £{gendered.loc[gendered['is_female']==0, 'price_num'].median():.2f}\")\n",
    "\n",
    "# Categories with both genders\n",
    "for level_name, level_col in [('broad', 'cat_broad'), ('mid', 'cat_mid'), ('fine', 'cat_fine')]:\n",
    "    cats_with_both = 0\n",
    "    for cat in gendered[level_col].unique():\n",
    "        sub = gendered[gendered[level_col] == cat]\n",
    "        if sub['is_female'].sum() >= 3 and (1 - sub['is_female']).sum() >= 3:\n",
    "            cats_with_both += 1\n",
    "    print(f\"  Categories ({level_name}) with ≥3F + ≥3M: {cats_with_both}\")\n",
    "\n",
    "# ============================================================================\n",
    "# REGRESSIONS\n",
    "# ============================================================================\n",
    "\n",
    "results_table = []\n",
    "\n",
    "def run_and_record(name, formula_or_result, data=None, controls='', coef_name='is_female',\n",
    "                   prefit=None):\n",
    "    \"\"\"Run OLS and record the female coefficient.\"\"\"\n",
    "    if prefit is not None:\n",
    "        model = prefit\n",
    "    else:\n",
    "        model = smf.ols(formula_or_result, data=data).fit(cov_type='HC1')\n",
    "\n",
    "    coef = model.params[coef_name]\n",
    "    se = model.bse[coef_name]\n",
    "    pval = model.pvalues[coef_name]\n",
    "    pct = (np.exp(coef) - 1) * 100\n",
    "    ci_lo = coef - 1.96 * se\n",
    "    ci_hi = coef + 1.96 * se\n",
    "\n",
    "    results_table.append({\n",
    "        'spec': name,\n",
    "        'coef': coef, 'se': se, 'p': pval,\n",
    "        'pct': pct,\n",
    "        'ci_lo': ci_lo, 'ci_hi': ci_hi,\n",
    "        'pct_lo': (np.exp(ci_lo) - 1) * 100,\n",
    "        'pct_hi': (np.exp(ci_hi) - 1) * 100,\n",
    "        'r2': model.rsquared,\n",
    "        'n': int(model.nobs),\n",
    "        'controls': controls,\n",
    "    })\n",
    "\n",
    "    sig = '***' if pval < 0.01 else ('**' if pval < 0.05 else ('*' if pval < 0.1 else ''))\n",
    "    print(f\"  {name}: coef={coef:+.4f} ({pct:+.1f}%), SE={se:.4f}, \"\n",
    "          f\"p={pval:.4f}{sig}, R²={model.rsquared:.3f}, N={int(model.nobs)}\")\n",
    "    return model\n",
    "\n",
    "# --- Spec 1: Raw ---\n",
    "print(\"\\nSpec 1: Raw gap\")\n",
    "spec1 = run_and_record('(1) Raw gap',\n",
    "                        'log_price ~ is_female', gendered, 'None')\n",
    "\n",
    "# --- Spec 2: + Store FE ---\n",
    "print(\"\\nSpec 2: + Store FE\")\n",
    "spec2 = run_and_record('(2) + Store FE',\n",
    "                        'log_price ~ is_female + C(store)', gendered, 'Store')\n",
    "\n",
    "# --- Spec 3: + Broad category FE ---\n",
    "# Filter to categories with ≥5 obs\n",
    "cat_counts_broad = gendered['cat_broad'].value_counts()\n",
    "valid_broad = cat_counts_broad[cat_counts_broad >= 5].index\n",
    "gen_broad = gendered[gendered['cat_broad'].isin(valid_broad)].copy()\n",
    "\n",
    "print(f\"\\nSpec 3: + Broad category FE (N cats: {len(valid_broad)})\")\n",
    "spec3 = run_and_record('(3) + Broad cat FE',\n",
    "                        'log_price ~ is_female + C(store) + C(cat_broad)',\n",
    "                        gen_broad, 'Store + Broad cat')\n",
    "\n",
    "# --- Spec 4: + Mid category FE ---\n",
    "cat_counts_mid = gendered['cat_mid'].value_counts()\n",
    "valid_mid = cat_counts_mid[cat_counts_mid >= 5].index\n",
    "gen_mid = gendered[gendered['cat_mid'].isin(valid_mid)].copy()\n",
    "\n",
    "print(f\"\\nSpec 4: + Mid category FE (N cats: {len(valid_mid)})\")\n",
    "spec4 = run_and_record('(4) + Mid cat FE',\n",
    "                        'log_price ~ is_female + C(store) + C(cat_mid)',\n",
    "                        gen_mid, 'Store + Mid cat')\n",
    "\n",
    "# --- Spec 5: + Fine category FE ---\n",
    "cat_counts_fine = gendered['cat_fine'].value_counts()\n",
    "valid_fine = cat_counts_fine[cat_counts_fine >= 5].index\n",
    "gen_fine = gendered[gendered['cat_fine'].isin(valid_fine)].copy()\n",
    "\n",
    "print(f\"\\nSpec 5: + Fine category FE (N cats: {len(valid_fine)})\")\n",
    "if len(valid_fine) > 0 and len(gen_fine) > 50:\n",
    "    spec5 = run_and_record('(5) + Fine cat FE',\n",
    "                            'log_price ~ is_female + C(store) + C(cat_fine)',\n",
    "                            gen_fine, 'Store + Fine cat')\n",
    "else:\n",
    "    print(\"  Skipped (too few categories)\")\n",
    "\n",
    "# --- Spec 6: + Description TF-IDF ---\n",
    "def clean_text_no_gender(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    for w in ALL_GENDER_KEYWORDS:\n",
    "        text = re.sub(r'\\b' + w + r'\\b', '', text)\n",
    "    return re.sub(r'\\s+', ' ', re.sub(r'[^a-z\\s]', ' ', text)).strip()\n",
    "\n",
    "gen_mid['desc_clean'] = gen_mid[COL_DESC].apply(clean_text_no_gender)\n",
    "\n",
    "desc_vec = TfidfVectorizer(max_features=100, min_df=5, max_df=0.9,\n",
    "                            ngram_range=(1, 2), stop_words='english')\n",
    "X_desc = desc_vec.fit_transform(gen_mid['desc_clean'])\n",
    "desc_names = [f'desc_{f}' for f in desc_vec.get_feature_names_out()]\n",
    "\n",
    "y6 = gen_mid['log_price'].values\n",
    "X6_parts = [\n",
    "    gen_mid[['is_female']].values,\n",
    "    pd.get_dummies(gen_mid['store'], prefix='store', drop_first=True).values,\n",
    "    pd.get_dummies(gen_mid['cat_mid'], prefix='cat', drop_first=True).values,\n",
    "    X_desc.toarray(),\n",
    "]\n",
    "X6 = sm.add_constant(np.hstack(X6_parts))\n",
    "\n",
    "spec6_model = sm.OLS(y6, X6).fit(cov_type='HC1')\n",
    "# is_female is column index 1 (after constant)\n",
    "coef6 = spec6_model.params[1]\n",
    "se6 = spec6_model.bse[1]\n",
    "pval6 = spec6_model.pvalues[1]\n",
    "results_table.append({\n",
    "    'spec': '(6) + Description',\n",
    "    'coef': coef6, 'se': se6, 'p': pval6,\n",
    "    'pct': (np.exp(coef6) - 1) * 100,\n",
    "    'ci_lo': coef6 - 1.96 * se6, 'ci_hi': coef6 + 1.96 * se6,\n",
    "    'pct_lo': (np.exp(coef6 - 1.96 * se6) - 1) * 100,\n",
    "    'pct_hi': (np.exp(coef6 + 1.96 * se6) - 1) * 100,\n",
    "    'r2': spec6_model.rsquared, 'n': int(spec6_model.nobs),\n",
    "    'controls': 'Store + Mid cat + Description',\n",
    "})\n",
    "sig6 = '***' if pval6 < 0.01 else ('**' if pval6 < 0.05 else ('*' if pval6 < 0.1 else ''))\n",
    "print(f\"\\nSpec 6: + Description TF-IDF\")\n",
    "print(f\"  (6) + Description: coef={coef6:+.4f} ({(np.exp(coef6)-1)*100:+.1f}%), \"\n",
    "      f\"SE={se6:.4f}, p={pval6:.4f}{sig6}, R²={spec6_model.rsquared:.3f}, N={int(spec6_model.nobs)}\")\n",
    "\n",
    "# --- Spec 7: Female × Store interaction ---\n",
    "print(f\"\\nSpec 7: Female × Store interaction\")\n",
    "spec7 = smf.ols('log_price ~ is_female * C(store) + C(cat_mid)',\n",
    "                 data=gen_mid).fit(cov_type='HC1')\n",
    "\n",
    "# Extract female coefficient and interaction terms\n",
    "print(f\"  Main effect (is_female): {spec7.params['is_female']:+.4f} \"\n",
    "      f\"(p={spec7.pvalues['is_female']:.4f})\")\n",
    "for param in spec7.params.index:\n",
    "    if 'is_female:' in param:\n",
    "        store_id = param.split('[T.')[1].rstrip(']')\n",
    "        sname = store_names.get(store_id, store_id)\n",
    "        total_effect = spec7.params['is_female'] + spec7.params[param]\n",
    "        pct_effect = (np.exp(total_effect) - 1) * 100\n",
    "        print(f\"  {sname}: total female effect = {total_effect:+.4f} ({pct_effect:+.1f}%), \"\n",
    "              f\"interaction p={spec7.pvalues[param]:.4f}\")\n",
    "\n",
    "# --- Spec 8: Unit price ---\n",
    "print(f\"\\nSpec 8: Unit price regression\")\n",
    "if 'unit_price_num' in gen_mid.columns:\n",
    "    gen_unit = gen_mid[gen_mid['unit_price_num'].notna() & (gen_mid['unit_price_num'] > 0)].copy()\n",
    "    gen_unit['log_unit_price'] = np.log(gen_unit['unit_price_num'])\n",
    "    if len(gen_unit) >= 50:\n",
    "        spec8 = run_and_record('(8) Unit price',\n",
    "                                'log_unit_price ~ is_female + C(store) + C(cat_mid)',\n",
    "                                gen_unit, 'Store + Mid cat (unit price)',\n",
    "                                coef_name='is_female')\n",
    "\n",
    "# --- Spec 9: Three-way (female, male, none) ---\n",
    "print(f\"\\nSpec 9: Three-way comparison (none = reference)\")\n",
    "df_valid = df.copy()\n",
    "df_valid['is_female'] = (df_valid['gender'] == 'female').astype(int)\n",
    "df_valid['is_male'] = (df_valid['gender'] == 'male').astype(int)\n",
    "\n",
    "spec9 = smf.ols('log_price ~ is_female + is_male + C(store)',\n",
    "                 data=df_valid).fit(cov_type='HC1')\n",
    "print(f\"  is_female: {spec9.params['is_female']:+.4f} (p={spec9.pvalues['is_female']:.4f})\")\n",
    "print(f\"  is_male:   {spec9.params['is_male']:+.4f} (p={spec9.pvalues['is_male']:.4f})\")\n",
    "f_vs_m = spec9.params['is_female'] - spec9.params['is_male']\n",
    "print(f\"  F vs M:    {(np.exp(f_vs_m)-1)*100:+.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# QUANTILE REGRESSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"QUANTILE REGRESSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quantiles = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "qreg_results = []\n",
    "\n",
    "for q in quantiles:\n",
    "    qmodel = smf.quantreg('log_price ~ is_female + C(store)', data=gendered).fit(q=q)\n",
    "    coef_q = qmodel.params['is_female']\n",
    "    se_q = qmodel.bse['is_female']\n",
    "    pval_q = qmodel.pvalues['is_female']\n",
    "    pct_q = (np.exp(coef_q) - 1) * 100\n",
    "\n",
    "    qreg_results.append({\n",
    "        'quantile': q,\n",
    "        'coef': coef_q,\n",
    "        'se': se_q,\n",
    "        'p': pval_q,\n",
    "        'pct': pct_q,\n",
    "        'ci_lo': coef_q - 1.96 * se_q,\n",
    "        'ci_hi': coef_q + 1.96 * se_q,\n",
    "    })\n",
    "\n",
    "    sig = '***' if pval_q < 0.01 else ('**' if pval_q < 0.05 else ('*' if pval_q < 0.1 else ''))\n",
    "    print(f\"  Q{q:.2f}: coef={coef_q:+.4f} ({pct_q:+.1f}%), p={pval_q:.4f}{sig}\")\n",
    "\n",
    "qreg_df = pd.DataFrame(qreg_results)\n",
    "\n",
    "# With category controls\n",
    "print(\"\\n  With mid-category controls:\")\n",
    "qreg_cat_results = []\n",
    "for q in quantiles:\n",
    "    try:\n",
    "        qmodel = smf.quantreg('log_price ~ is_female + C(store) + C(cat_mid)',\n",
    "                                data=gen_mid).fit(q=q, max_iter=5000)\n",
    "        coef_q = qmodel.params['is_female']\n",
    "        se_q = qmodel.bse['is_female']\n",
    "        pval_q = qmodel.pvalues['is_female']\n",
    "        pct_q = (np.exp(coef_q) - 1) * 100\n",
    "        qreg_cat_results.append({\n",
    "            'quantile': q, 'coef': coef_q, 'se': se_q,\n",
    "            'p': pval_q, 'pct': pct_q,\n",
    "            'ci_lo': coef_q - 1.96 * se_q,\n",
    "            'ci_hi': coef_q + 1.96 * se_q,\n",
    "        })\n",
    "        sig = '***' if pval_q < 0.01 else ('**' if pval_q < 0.05 else ('*' if pval_q < 0.1 else ''))\n",
    "        print(f\"  Q{q:.2f}: coef={coef_q:+.4f} ({pct_q:+.1f}%), p={pval_q:.4f}{sig}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Q{q:.2f}: failed ({e})\")\n",
    "\n",
    "qreg_cat_df = pd.DataFrame(qreg_cat_results) if qreg_cat_results else pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# WITHIN-CATEGORY ANALYSIS (with bootstrap CIs)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WITHIN-CATEGORY ANALYSIS (bootstrap CIs)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "category_gaps = []\n",
    "\n",
    "for cat in gendered['cat_mid'].unique():\n",
    "    sub = gendered[gendered['cat_mid'] == cat]\n",
    "    fem = sub[sub['is_female'] == 1]['price_num']\n",
    "    mal = sub[sub['is_female'] == 0]['price_num']\n",
    "\n",
    "    if len(fem) >= 3 and len(mal) >= 3:\n",
    "        gap_pct = (fem.mean() / mal.mean() - 1) * 100\n",
    "        log_gap = np.log(fem).mean() - np.log(mal).mean()\n",
    "\n",
    "        # Bootstrap CI\n",
    "        boot_gaps = []\n",
    "        for _ in range(N_BOOTSTRAP):\n",
    "            f_boot = rng.choice(fem.values, size=len(fem), replace=True)\n",
    "            m_boot = rng.choice(mal.values, size=len(mal), replace=True)\n",
    "            if m_boot.mean() > 0:\n",
    "                boot_gaps.append((f_boot.mean() / m_boot.mean() - 1) * 100)\n",
    "\n",
    "        ci_lo = np.percentile(boot_gaps, 2.5) if boot_gaps else np.nan\n",
    "        ci_hi = np.percentile(boot_gaps, 97.5) if boot_gaps else np.nan\n",
    "\n",
    "        category_gaps.append({\n",
    "            'category': cat,\n",
    "            'n_female': len(fem),\n",
    "            'n_male': len(mal),\n",
    "            'n_total': len(fem) + len(mal),\n",
    "            'mean_female': fem.mean(),\n",
    "            'mean_male': mal.mean(),\n",
    "            'gap_pct': gap_pct,\n",
    "            'log_gap': log_gap,\n",
    "            'ci_lo': ci_lo,\n",
    "            'ci_hi': ci_hi,\n",
    "            'significant': (ci_lo > 0 and ci_hi > 0) or (ci_lo < 0 and ci_hi < 0),\n",
    "        })\n",
    "\n",
    "gaps_df = pd.DataFrame(category_gaps).sort_values('gap_pct', ascending=False)\n",
    "\n",
    "if len(gaps_df) > 0:\n",
    "    print(f\"Categories with both genders (≥3 each): {len(gaps_df)}\")\n",
    "\n",
    "    # Trimmed averages (drop top/bottom outlier)\n",
    "    gaps_trimmed = gaps_df.copy()\n",
    "    if len(gaps_trimmed) > 4:\n",
    "        gaps_trimmed = gaps_trimmed.iloc[1:-1]  # drop most extreme\n",
    "\n",
    "    weighted_gap = np.average(gaps_df['gap_pct'], weights=gaps_df['n_total'])\n",
    "    trimmed_mean = gaps_trimmed['gap_pct'].mean()\n",
    "    median_gap = gaps_df['gap_pct'].median()\n",
    "\n",
    "    print(f\"\\nWithin-category price gap (F vs M):\")\n",
    "    print(f\"  Weighted mean: {weighted_gap:+.1f}%\")\n",
    "    print(f\"  Trimmed mean:  {trimmed_mean:+.1f}%\")\n",
    "    print(f\"  Median:        {median_gap:+.1f}%\")\n",
    "\n",
    "    print(f\"\\n  {'category':<45s} {'F':>3s} {'M':>3s} {'gap':>8s} {'95% CI':>16s} {'sig':>4s}\")\n",
    "    print(f\"  {'-'*80}\")\n",
    "    for _, row in gaps_df.iterrows():\n",
    "        sig = '*' if row['significant'] else ''\n",
    "        print(f\"  {row['category'][:45]:<45s} {row['n_female']:>3.0f} {row['n_male']:>3.0f} \"\n",
    "              f\"{row['gap_pct']:>+7.1f}% [{row['ci_lo']:>+6.1f}, {row['ci_hi']:>+6.1f}] {sig:>3s}\")\n",
    "\n",
    "    gaps_df.to_csv(OUTPUT_DIR / 'within_category_gaps.csv', index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# BY-STORE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PINK TAX BY STORE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "store_results = []\n",
    "for store_id in gendered['store'].unique():\n",
    "    sub = gendered[gendered['store'] == store_id]\n",
    "    fem = sub[sub['is_female'] == 1]\n",
    "    mal = sub[sub['is_female'] == 0]\n",
    "    sname = store_names.get(store_id, store_id)\n",
    "\n",
    "    if len(fem) >= 10 and len(mal) >= 10:\n",
    "        model = smf.ols('log_price ~ is_female', data=sub).fit(cov_type='HC1')\n",
    "        coef = model.params['is_female']\n",
    "        pval = model.pvalues['is_female']\n",
    "        store_results.append({\n",
    "            'store': sname,\n",
    "            'store_id': store_id,\n",
    "            'n_female': len(fem), 'n_male': len(mal),\n",
    "            'mean_f': fem['price_num'].mean(),\n",
    "            'mean_m': mal['price_num'].mean(),\n",
    "            'coef': coef,\n",
    "            'pct_gap': (np.exp(coef) - 1) * 100,\n",
    "            'p_value': pval,\n",
    "            'significant': pval < 0.05,\n",
    "        })\n",
    "\n",
    "store_df = pd.DataFrame(store_results).sort_values('pct_gap', ascending=False)\n",
    "\n",
    "if len(store_df) > 0:\n",
    "    print(f\"{'store':<15s} {'F':>5s} {'M':>5s} {'mean F':>8s} {'mean M':>8s} {'gap':>8s} {'p':>8s}\")\n",
    "    print(f\"{'-'*65}\")\n",
    "    for _, row in store_df.iterrows():\n",
    "        sig = '***' if row['p_value'] < 0.01 else ('**' if row['p_value'] < 0.05 else\n",
    "              ('*' if row['p_value'] < 0.1 else ''))\n",
    "        print(f\"{row['store']:<15s} {row['n_female']:>5.0f} {row['n_male']:>5.0f} \"\n",
    "              f\"£{row['mean_f']:>6.2f} £{row['mean_m']:>6.2f} \"\n",
    "              f\"{row['pct_gap']:>+7.1f}% {row['p_value']:>7.4f}{sig}\")\n",
    "\n",
    "    store_df.to_csv(OUTPUT_DIR / 'pink_tax_by_store.csv', index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS SUMMARY TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"REGRESSION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_df = pd.DataFrame(results_table)\n",
    "print()\n",
    "print(f\"{'Spec':<25s} {'Coef':>8s} {'%gap':>8s} {'95% CI':>18s} {'p':>8s} {'R²':>6s} {'N':>6s}\")\n",
    "print(f\"{'-'*80}\")\n",
    "for _, row in summary_df.iterrows():\n",
    "    sig = '***' if row['p'] < 0.01 else ('**' if row['p'] < 0.05 else ('*' if row['p'] < 0.1 else ''))\n",
    "    print(f\"{row['spec']:<25s} {row['coef']:>+7.4f} {row['pct']:>+7.1f}% \"\n",
    "          f\"[{row['pct_lo']:>+6.1f}, {row['pct_hi']:>+6.1f}] \"\n",
    "          f\"{row['p']:>7.4f}{sig:<3s} {row['r2']:>5.3f} {row['n']:>6.0f}\")\n",
    "\n",
    "summary_df.to_csv(OUTPUT_DIR / 'regression_summary.csv', index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALISATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GENERATING CHARTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ---- 1. Coefficient plot across specifications ----\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "specs = summary_df['spec'].values\n",
    "coefs = summary_df['coef'].values\n",
    "ci_los = summary_df['ci_lo'].values\n",
    "ci_his = summary_df['ci_hi'].values\n",
    "\n",
    "y_pos = np.arange(len(specs))\n",
    "xerr_lo = coefs - ci_los\n",
    "xerr_hi = ci_his - coefs\n",
    "\n",
    "colors = [PALETTE['female'] if c > 0 else PALETTE['male'] for c in coefs]\n",
    "\n",
    "ax.barh(y_pos, coefs, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5, height=0.6)\n",
    "ax.errorbar(coefs, y_pos, xerr=[xerr_lo, xerr_hi], fmt='none', color='black',\n",
    "            capsize=4, linewidth=1.2)\n",
    "ax.axvline(x=0, color='black', linewidth=1, linestyle='-')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(specs)\n",
    "ax.set_xlabel('Coefficient on female indicator (log price)')\n",
    "ax.set_title('Female price premium: sensitivity to controls')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, (c, p) in enumerate(zip(coefs, summary_df['pct'].values)):\n",
    "    offset = max(abs(ci_his[i]), abs(ci_los[i])) + 0.01\n",
    "    ax.text(ci_his[i] + 0.01, i, f'{p:+.1f}%', va='center', ha='left', fontsize=9,\n",
    "            fontweight='bold' if summary_df.iloc[i]['p'] < 0.05 else 'normal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '01_coefficient_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"✓ 01_coefficient_plot.png\")\n",
    "\n",
    "# ---- 2. Quantile regression ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Without category controls\n",
    "ax = axes[0]\n",
    "qs = qreg_df['quantile']\n",
    "ax.fill_between(qs, qreg_df['ci_lo'], qreg_df['ci_hi'], alpha=0.2, color=PALETTE['female'])\n",
    "ax.plot(qs, qreg_df['coef'], 'o-', color=PALETTE['female'], linewidth=2, markersize=6)\n",
    "ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--')\n",
    "ax.axhline(y=spec1.params['is_female'], color='gray', linewidth=0.8, linestyle=':',\n",
    "           label=f'OLS mean ({(np.exp(spec1.params[\"is_female\"])-1)*100:+.1f}%)')\n",
    "ax.set_xlabel('Quantile')\n",
    "ax.set_ylabel('Coefficient on female (log price)')\n",
    "ax.set_title('Quantile regression: store controls only')\n",
    "ax.set_xticks(quantiles)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# With category controls\n",
    "if len(qreg_cat_df) > 0:\n",
    "    ax = axes[1]\n",
    "    qs_c = qreg_cat_df['quantile']\n",
    "    ax.fill_between(qs_c, qreg_cat_df['ci_lo'], qreg_cat_df['ci_hi'],\n",
    "                    alpha=0.2, color=PALETTE['female'])\n",
    "    ax.plot(qs_c, qreg_cat_df['coef'], 'o-', color=PALETTE['female'], linewidth=2, markersize=6)\n",
    "    ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--')\n",
    "    ax.set_xlabel('Quantile')\n",
    "    ax.set_ylabel('Coefficient on female (log price)')\n",
    "    ax.set_title('Quantile regression: store + category controls')\n",
    "    ax.set_xticks(quantiles)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '02_quantile_regression.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"✓ 02_quantile_regression.png\")\n",
    "\n",
    "# ---- 3. Within-category gaps with CIs ----\n",
    "if len(gaps_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, max(5, len(gaps_df) * 0.5)))\n",
    "\n",
    "    gaps_sorted = gaps_df.sort_values('gap_pct')\n",
    "    y_pos = np.arange(len(gaps_sorted))\n",
    "\n",
    "    colors = [PALETTE['female'] if g > 0 else PALETTE['male'] for g in gaps_sorted['gap_pct']]\n",
    "    edge = ['black' if s else 'gray' for s in gaps_sorted['significant']]\n",
    "\n",
    "    bars = ax.barh(y_pos, gaps_sorted['gap_pct'], color=colors, alpha=0.6, height=0.7)\n",
    "    for bar, ec in zip(bars, edge):\n",
    "        bar.set_edgecolor(ec)\n",
    "        bar.set_linewidth(1 if ec == 'black' else 0.3)\n",
    "\n",
    "    # CI whiskers\n",
    "    ax.errorbar(gaps_sorted['gap_pct'].values, y_pos,\n",
    "                xerr=[gaps_sorted['gap_pct'].values - gaps_sorted['ci_lo'].values,\n",
    "                      gaps_sorted['ci_hi'].values - gaps_sorted['gap_pct'].values],\n",
    "                fmt='none', color='black', capsize=3, linewidth=0.8)\n",
    "\n",
    "    ax.axvline(x=0, color='black', linewidth=1)\n",
    "    ax.set_yticks(y_pos)\n",
    "    labels = [f\"{r['category'][:42]} (F:{r['n_female']:.0f}, M:{r['n_male']:.0f})\"\n",
    "              for _, r in gaps_sorted.iterrows()]\n",
    "    ax.set_yticklabels(labels, fontsize=8)\n",
    "    ax.set_xlabel('Female price premium (%)')\n",
    "    ax.set_title('Within-category price gaps with 95% bootstrap CIs')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / '03_within_category_gaps.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ 03_within_category_gaps.png\")\n",
    "\n",
    "# ---- 4. Price distributions ----\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 4a: Histogram\n",
    "ax = axes[0, 0]\n",
    "for gender, color in [('female', PALETTE['female']), ('male', PALETTE['male'])]:\n",
    "    sub = gendered[gendered['gender'] == gender]\n",
    "    ax.hist(sub['price_num'], bins=50, alpha=0.5, color=color,\n",
    "            label=f'{gender.title()} (n={len(sub)})', density=True)\n",
    "ax.set_xlabel('Price (£)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Price distributions')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, gendered['price_num'].quantile(0.95))\n",
    "\n",
    "# 4b: Log price\n",
    "ax = axes[0, 1]\n",
    "for gender, color in [('female', PALETTE['female']), ('male', PALETTE['male'])]:\n",
    "    sub = gendered[gendered['gender'] == gender]\n",
    "    ax.hist(sub['log_price'], bins=50, alpha=0.5, color=color,\n",
    "            label=f'{gender.title()} (n={len(sub)})', density=True)\n",
    "ax.set_xlabel('Log price')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Log price distributions')\n",
    "ax.legend()\n",
    "\n",
    "# 4c: Box plots\n",
    "ax = axes[1, 0]\n",
    "data_box = [gendered[gendered['gender'] == 'female']['price_num'],\n",
    "            gendered[gendered['gender'] == 'male']['price_num']]\n",
    "bp = ax.boxplot(data_box, labels=['Female', 'Male'], patch_artist=True,\n",
    "                showfliers=False, widths=0.5)\n",
    "bp['boxes'][0].set_facecolor(PALETTE['female'])\n",
    "bp['boxes'][1].set_facecolor(PALETTE['male'])\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.6)\n",
    "ax.set_ylabel('Price (£)')\n",
    "ax.set_title('Price box plots (outliers trimmed)')\n",
    "\n",
    "# 4d: CDF\n",
    "ax = axes[1, 1]\n",
    "for gender, color in [('female', PALETTE['female']), ('male', PALETTE['male'])]:\n",
    "    sub = gendered[gendered['gender'] == gender]['price_num'].sort_values()\n",
    "    cdf = np.arange(1, len(sub) + 1) / len(sub)\n",
    "    ax.plot(sub, cdf, color=color, label=gender.title(), linewidth=1.5)\n",
    "ax.set_xlabel('Price (£)')\n",
    "ax.set_ylabel('Cumulative probability')\n",
    "ax.set_title('Cumulative distribution functions')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, gendered['price_num'].quantile(0.95))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '04_price_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"✓ 04_price_distributions.png\")\n",
    "\n",
    "# ---- 5. By store ----\n",
    "if len(store_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # 5a: Coefficient bar chart\n",
    "    ax = axes[0]\n",
    "    store_sorted = store_df.sort_values('pct_gap')\n",
    "    y_pos = np.arange(len(store_sorted))\n",
    "    colors = [PALETTE['female'] if g > 0 else PALETTE['male'] for g in store_sorted['pct_gap']]\n",
    "    edge_w = [2 if s else 0.5 for s in store_sorted['significant']]\n",
    "\n",
    "    bars = ax.barh(y_pos, store_sorted['pct_gap'], color=colors, alpha=0.7, height=0.5)\n",
    "    for bar, lw in zip(bars, edge_w):\n",
    "        bar.set_edgecolor('black')\n",
    "        bar.set_linewidth(lw)\n",
    "\n",
    "    ax.axvline(x=0, color='black', linewidth=1)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(store_sorted['store'].values)\n",
    "    ax.set_xlabel('Female price premium (%)')\n",
    "    ax.set_title('Pink tax by store (thick border = p<0.05)')\n",
    "\n",
    "    # 5b: Mean prices by store and gender\n",
    "    ax = axes[1]\n",
    "    x = np.arange(len(store_df))\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, store_df['mean_f'], width, color=PALETTE['female'],\n",
    "           alpha=0.7, label='Female', edgecolor='black', linewidth=0.3)\n",
    "    ax.bar(x + width/2, store_df['mean_m'], width, color=PALETTE['male'],\n",
    "           alpha=0.7, label='Male', edgecolor='black', linewidth=0.3)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(store_df['store'].values)\n",
    "    ax.set_ylabel('Mean price (£)')\n",
    "    ax.set_title('Mean prices by store and gender')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / '05_by_store.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ 05_by_store.png\")\n",
    "\n",
    "# ---- 6. Female vs male scatter by category ----\n",
    "if len(gaps_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    ax.scatter(gaps_df['mean_male'], gaps_df['mean_female'],\n",
    "               s=gaps_df['n_total'] * 5, alpha=0.6,\n",
    "               c=[PALETTE['female'] if g > 0 else PALETTE['male'] for g in gaps_df['gap_pct']],\n",
    "               edgecolors='black', linewidth=0.5)\n",
    "\n",
    "    # 45-degree line\n",
    "    lim_max = max(gaps_df['mean_male'].max(), gaps_df['mean_female'].max()) * 1.1\n",
    "    ax.plot([0, lim_max], [0, lim_max], 'k--', linewidth=0.8, alpha=0.5, label='Equal price')\n",
    "    ax.set_xlabel('Mean male price (£)')\n",
    "    ax.set_ylabel('Mean female price (£)')\n",
    "    ax.set_title('Female vs male mean prices by category\\n(size = total products)')\n",
    "    ax.legend()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Label the outliers\n",
    "    for _, row in gaps_df.nlargest(3, 'gap_pct').iterrows():\n",
    "        ax.annotate(row['category'][:30], (row['mean_male'], row['mean_female']),\n",
    "                    fontsize=7, alpha=0.8,\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / '06_scatter_by_category.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ 06_scatter_by_category.png\")\n",
    "\n",
    "# ---- 7. Three-way comparison (female, male, none) ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# 7a: Density by gender group\n",
    "ax = axes[0]\n",
    "for gender, color in [('female', PALETTE['female']), ('male', PALETTE['male']),\n",
    "                       ('none', PALETTE['none'])]:\n",
    "    sub = df[df['gender'] == gender]\n",
    "    ax.hist(sub['log_price'], bins=60, alpha=0.4, color=color, density=True,\n",
    "            label=f'{gender.title()} (n={len(sub):,})')\n",
    "ax.set_xlabel('Log price')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Price distributions: all gender groups')\n",
    "ax.legend()\n",
    "\n",
    "# 7b: Box plots\n",
    "ax = axes[1]\n",
    "groups = ['female', 'male', 'none']\n",
    "data_3way = [df[df['gender'] == g]['price_num'] for g in groups]\n",
    "bp = ax.boxplot(data_3way, labels=[g.title() for g in groups],\n",
    "                patch_artist=True, showfliers=False, widths=0.5)\n",
    "for i, box in enumerate(bp['boxes']):\n",
    "    box.set_facecolor(PALETTE[groups[i]])\n",
    "    box.set_alpha(0.6)\n",
    "ax.set_ylabel('Price (£)')\n",
    "ax.set_title('Price comparison: gendered vs ungendered')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '07_three_way_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"✓ 07_three_way_comparison.png\")\n",
    "\n",
    "# ---- 8. Category composition ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 8a: Top categories by gender\n",
    "for ax_idx, (gender, title) in enumerate([('female', 'Female'), ('male', 'Male')]):\n",
    "    ax = axes[ax_idx]\n",
    "    sub = gendered[gendered['gender'] == gender]\n",
    "    top_cats = sub['cat_mid'].value_counts().head(12)\n",
    "    y_pos = np.arange(len(top_cats))\n",
    "    ax.barh(y_pos, top_cats.values, color=PALETTE[gender], alpha=0.7,\n",
    "            edgecolor='black', linewidth=0.3)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([c[:40] for c in top_cats.index], fontsize=8)\n",
    "    ax.set_xlabel('Number of products')\n",
    "    ax.set_title(f'Top categories: {title} products')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '08_category_composition.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"✓ 08_category_composition.png\")\n",
    "\n",
    "# ---- 9. Price ratio distribution within matched categories ----\n",
    "if len(gaps_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    gap_values = gaps_df['gap_pct'].values\n",
    "    colors = [PALETTE['female'] if g > 0 else PALETTE['male'] for g in gap_values]\n",
    "\n",
    "    ax.hist(gap_values, bins=max(5, len(gap_values) // 2), alpha=0.6,\n",
    "            color=PALETTE['female'], edgecolor='black', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='black', linewidth=1.5)\n",
    "    ax.axvline(x=gaps_df['gap_pct'].median(), color=PALETTE['female'],\n",
    "               linewidth=1.5, linestyle='--',\n",
    "               label=f'Median: {gaps_df[\"gap_pct\"].median():+.1f}%')\n",
    "    ax.set_xlabel('Female price premium (%)')\n",
    "    ax.set_ylabel('Number of categories')\n",
    "    ax.set_title('Distribution of within-category price gaps')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / '09_gap_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ 09_gap_distribution.png\")\n",
    "\n",
    "# ---- 10. R² progression ----\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "r2_data = summary_df[['spec', 'r2']].copy()\n",
    "y_pos = np.arange(len(r2_data))\n",
    "\n",
    "ax.barh(y_pos, r2_data['r2'], color='#555555', alpha=0.7,\n",
    "        edgecolor='black', linewidth=0.5, height=0.6)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(r2_data['spec'])\n",
    "ax.set_xlabel('R²')\n",
    "ax.set_title('Model fit: how much variance do controls explain?')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, r2 in enumerate(r2_data['r2']):\n",
    "    ax.text(r2 + 0.01, i, f'{r2:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '10_r2_progression.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"✓ 10_r2_progression.png\")\n",
    "\n",
    "# ---- 11. Heatmap: category × store price gap ----\n",
    "# Only for categories present in ≥2 stores with both genders\n",
    "heatmap_data = []\n",
    "for cat in gendered['cat_mid'].unique():\n",
    "    for store_id in gendered['store'].unique():\n",
    "        sub = gendered[(gendered['cat_mid'] == cat) & (gendered['store'] == store_id)]\n",
    "        fem = sub[sub['is_female'] == 1]\n",
    "        mal = sub[sub['is_female'] == 0]\n",
    "        if len(fem) >= 2 and len(mal) >= 2:\n",
    "            gap = (fem['price_num'].mean() / mal['price_num'].mean() - 1) * 100\n",
    "            heatmap_data.append({\n",
    "                'category': cat[:35],\n",
    "                'store': store_names.get(store_id, store_id),\n",
    "                'gap': gap,\n",
    "                'n': len(fem) + len(mal),\n",
    "            })\n",
    "\n",
    "if heatmap_data:\n",
    "    heat_df = pd.DataFrame(heatmap_data)\n",
    "    pivot = heat_df.pivot_table(values='gap', index='category', columns='store', aggfunc='mean')\n",
    "\n",
    "    # Only keep categories that appear in at least 2 stores\n",
    "    pivot = pivot.dropna(thresh=2)\n",
    "\n",
    "    if len(pivot) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(8, max(5, len(pivot) * 0.45)))\n",
    "        sns.heatmap(pivot, cmap='RdBu_r', center=0, annot=True, fmt='.0f',\n",
    "                    linewidths=0.5, ax=ax, cbar_kws={'label': 'F vs M gap (%)'})\n",
    "        ax.set_title('Price gap (%) by category and store')\n",
    "        ax.set_ylabel('')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR / '11_heatmap_category_store.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"✓ 11_heatmap_category_store.png\")\n",
    "\n",
    "# ---- 12. Summary dashboard ----\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "gs = gridspec.GridSpec(2, 3, figure=fig, hspace=0.4, wspace=0.35)\n",
    "\n",
    "# 12a: Key finding\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "ax.axis('off')\n",
    "ax.text(0.5, 0.85, 'KEY FINDING', ha='center', fontsize=14, fontweight='bold')\n",
    "ax.text(0.5, 0.60, f'Raw gap: {(np.exp(spec1.params[\"is_female\"])-1)*100:+.1f}%',\n",
    "        ha='center', fontsize=18, color=PALETTE['male'], fontweight='bold')\n",
    "ax.text(0.5, 0.40, '(female products cheaper)', ha='center', fontsize=10, color='gray')\n",
    "ax.text(0.5, 0.15, f'After controls: {summary_df.iloc[-1][\"pct\"]:+.1f}%\\n(not significant)',\n",
    "        ha='center', fontsize=12, color='gray')\n",
    "\n",
    "# 12b: Coefficient mini-plot\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "y_pos = np.arange(len(summary_df))\n",
    "colors = [PALETTE['female'] if c > 0 else PALETTE['male'] for c in summary_df['coef']]\n",
    "ax.barh(y_pos, summary_df['pct'], color=colors, alpha=0.7, height=0.6)\n",
    "ax.axvline(x=0, color='black', linewidth=1)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([s[:18] for s in summary_df['spec']], fontsize=8)\n",
    "ax.set_xlabel('% gap')\n",
    "ax.set_title('Premium by spec')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# 12c: R² bar\n",
    "ax = fig.add_subplot(gs[0, 2])\n",
    "ax.barh(y_pos, summary_df['r2'], color='#666', alpha=0.7, height=0.6)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([s[:18] for s in summary_df['spec']], fontsize=8)\n",
    "ax.set_xlabel('R²')\n",
    "ax.set_title('Model fit')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# 12d: Price distributions\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "for gender, color in [('female', PALETTE['female']), ('male', PALETTE['male'])]:\n",
    "    sub = gendered[gendered['gender'] == gender]\n",
    "    ax.hist(sub['log_price'], bins=40, alpha=0.5, color=color, density=True, label=gender.title())\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlabel('Log price')\n",
    "ax.set_title('Price distributions')\n",
    "\n",
    "# 12e: Quantile regression\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "ax.fill_between(qreg_df['quantile'], qreg_df['ci_lo'], qreg_df['ci_hi'],\n",
    "                alpha=0.2, color=PALETTE['female'])\n",
    "ax.plot(qreg_df['quantile'], qreg_df['coef'], 'o-', color=PALETTE['female'], linewidth=2)\n",
    "ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--')\n",
    "ax.set_xlabel('Quantile')\n",
    "ax.set_ylabel('Coefficient')\n",
    "ax.set_title('Quantile regression')\n",
    "\n",
    "# 12f: By store\n",
    "if len(store_df) > 0:\n",
    "    ax = fig.add_subplot(gs[1, 2])\n",
    "    y_pos = np.arange(len(store_df))\n",
    "    colors = [PALETTE['female'] if g > 0 else PALETTE['male'] for g in store_df['pct_gap']]\n",
    "    ax.barh(y_pos, store_df['pct_gap'], color=colors, alpha=0.7, height=0.5)\n",
    "    ax.axvline(x=0, color='black', linewidth=1)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(store_df['store'].values, fontsize=9)\n",
    "    ax.set_xlabel('% gap')\n",
    "    ax.set_title('Gap by store')\n",
    "\n",
    "plt.savefig(OUTPUT_DIR / '12_summary_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"✓ 12_summary_dashboard.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL OUTPUT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OUTPUT FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_files = sorted(OUTPUT_DIR.glob('*'))\n",
    "for f in all_files:\n",
    "    print(f\"  {f.name}\")\n",
    "\n",
    "# Save full summary\n",
    "full_summary = {\n",
    "    'raw_gap_pct': float((np.exp(spec1.params['is_female']) - 1) * 100),\n",
    "    'raw_gap_p': float(spec1.pvalues['is_female']),\n",
    "    'controlled_gap_pct': float(summary_df.iloc[-1]['pct']),\n",
    "    'controlled_gap_p': float(summary_df.iloc[-1]['p']),\n",
    "    'within_category_median_gap': float(median_gap) if len(gaps_df) > 0 else None,\n",
    "    'n_gendered_products': int(len(gendered)),\n",
    "    'n_female': int(gendered['is_female'].sum()),\n",
    "    'n_male': int((1 - gendered['is_female']).sum()),\n",
    "    'quantile_results': qreg_df.to_dict('records'),\n",
    "    'store_results': store_df.to_dict('records') if len(store_df) > 0 else [],\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'full_summary.json', 'w') as f:\n",
    "    json.dump(full_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✓ All outputs saved to {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
